{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "import ast\n",
    "\n",
    "import time\n",
    "\n",
    "import wfdb.processing\n",
    "import wfdb.processing.evaluate\n",
    "import wfdb.processing.qrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Methods for data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method defined by physionet to load data\n",
    "def load_raw_data(df, sampling_rate, path):\n",
    "    # Loading all data with signal and meta information\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    \n",
    "    # Eliminating meta information. We are selecting only signal value of 12 leads \n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"D:\\SynologyDrive\\10_Arbeit_und_Bildung\\20_Masterstudium\\01_Semester\\90_Projekt\\10_DEV\"\n",
    "path = base_path + \"/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/\"\n",
    "\n",
    "features_by_ecg_id = pd.read_csv(path+'ptbxl_database.csv', index_col='ecg_id')\n",
    "features_by_ecg_id.scp_codes = features_by_ecg_id.scp_codes.apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_features_train = []\n",
    "data_with_features_test = []\n",
    "data_with_features_validation = []\n",
    "\n",
    "save_all = False\n",
    "enable_plot = False\t\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(time.time_ns()%10000) # Set seed for reproducibility\n",
    "\n",
    "for i in range(0, 1000): # len(features_by_ecg_id)\n",
    "    \n",
    "    # Load raw data\n",
    "    features_by_ecg_id_selected = features_by_ecg_id.iloc[i:i+1]\n",
    "    raw_data_row_i = load_raw_data(features_by_ecg_id_selected, 500, path)[0]\n",
    "    \n",
    "    # Calculate the median lead of 12-lead-ecg\n",
    "    median_lead = np.transpose(np.median(np.transpose(raw_data_row_i), axis=0))\n",
    "    # Normalize median lead\n",
    "    median_lead = (median_lead - np.mean(median_lead)) / np.std(median_lead)\n",
    "\n",
    "    # Calculate the R-peaks\n",
    "    rpeaks = wfdb.processing.xqrs_detect(median_lead, fs=500, verbose=False)\n",
    "\n",
    "    # Generate feature vector and fill it with zeros, then fill it with 1 at the R-peak positions\n",
    "    feature_rpeak = np.zeros(len(median_lead))\n",
    "    feature_rpeak[rpeaks.astype(int)] = 1\n",
    "\n",
    "    # Generate time id (0-4999) for each sample\n",
    "    time_idx = np.arange(0, len(median_lead))\n",
    "    # Convert data type of time_idx to int\n",
    "    time_idx = time_idx.astype(int)\n",
    "    \n",
    "\n",
    "    # Build Pandas DataFrame containing raw data and features\n",
    "    df = pd.DataFrame({'time_idx': time_idx, 'raw_data': median_lead, 'feature_rpeak': feature_rpeak})\n",
    "\n",
    "    # Use random number to define if the data is used for training or testing or validation\n",
    "    random_number = np.random.rand()\n",
    "    if random_number < 0.7:\n",
    "        data_with_features_train.append(df)\n",
    "    elif random_number >= 0.7 and random_number < 0.9:\n",
    "        data_with_features_test.append(df)\n",
    "    else:\n",
    "        data_with_features_validation.append(df)\n",
    "\n",
    "    if( enable_plot ):\n",
    "        # Print with matplotlib\n",
    "        plt.plot(median_lead)\n",
    "        plt.plot(feature_rpeak)\n",
    "        # Make the plot larger\n",
    "        plt.gcf().set_size_inches(20, 10)\n",
    "        plt.show()\n",
    "\n",
    "if not save_all:\n",
    "    # Delete folders and files\n",
    "    import shutil\n",
    "    shutil.rmtree(base_path + \"/data/pd_dataset_train\", ignore_errors=True)\n",
    "    shutil.rmtree(base_path + \"/data/pd_dataset_test\", ignore_errors=True)\n",
    "    shutil.rmtree(base_path + \"/data/pd_dataset_val\", ignore_errors=True)\n",
    "    # Generate Same structure again\n",
    "    import os\n",
    "    os.makedirs(base_path + \"/data/pd_dataset_train\")\n",
    "    os.makedirs(base_path + \"/data/pd_dataset_test\")\n",
    "    os.makedirs(base_path + \"/data/pd_dataset_val\")\n",
    "\n",
    "    # Iterate through all elements in data_with_features_train\n",
    "    for i in range(0, len(data_with_features_train)):\n",
    "        # Select a random starting point\n",
    "        start_idx = np.random.randint(0, len(data_with_features_train[i]) - 512)\n",
    "        # Extract 512 datapoints\n",
    "        pd_dataset_train = data_with_features_train[i].iloc[start_idx:start_idx + 512]\n",
    "\n",
    "        # Save the data to a file\n",
    "        pd_dataset_train.to_csv(base_path + \"/data/pd_dataset_train/\" + str(i) + \".csv\", index=False)\n",
    "\n",
    "    # Iterate through all elements in data_with_features_test\n",
    "    for i in range(0, len(data_with_features_test)):\n",
    "        # Select a random starting point\n",
    "        start_idx = np.random.randint(0, len(data_with_features_test[i]) - 512)\n",
    "        # Extract 512 datapoints\n",
    "        pd_dataset_test = data_with_features_test[i].iloc[start_idx:start_idx + 512]\n",
    "\n",
    "        # Save the data to a file\n",
    "        pd_dataset_test.to_csv(base_path + \"/data/pd_dataset_test/\" + str(i) + \".csv\", index=False)\n",
    "\n",
    "    # Iterate through all elements in data_with_features_validation\n",
    "    for i in range(0, len(data_with_features_validation)):\n",
    "        # Select a random starting point\n",
    "        start_idx = np.random.randint(0, len(data_with_features_validation[i]) - 512)\n",
    "        # Extract 512 datapoints\n",
    "        pd_dataset_validation = data_with_features_validation[i].iloc[start_idx:start_idx + 512]\n",
    "\n",
    "        # Save the data to a file\n",
    "        pd_dataset_validation.to_csv(base_path + \"/data/pd_dataset_val/\" + str(i) + \".csv\", index=False)\n",
    "\n",
    "if save_all:\n",
    "    # Convert List to DataFrame but segment it by using a column called group_id\n",
    "    pd_dataset_train = pd.concat(data_with_features_train)\n",
    "    pd_dataset_test = pd.concat(data_with_features_test)\n",
    "    pd_dataset_validation = pd.concat(data_with_features_validation)\n",
    "\n",
    "    # Delete variables that are not needed anymore\n",
    "    del data_with_features_train\n",
    "    del data_with_features_test\n",
    "    del data_with_features_validation\n",
    "\n",
    "    # Add a column to the DataFrame that segments the data into groups of 5000 samples\n",
    "    pd_dataset_train['group_ids'] = np.repeat(np.arange(0, len(pd_dataset_train)/5000), 5000)\n",
    "    pd_dataset_test['group_ids'] = np.repeat(np.arange(0, len(pd_dataset_test)/5000), 5000)\n",
    "    pd_dataset_validation['group_ids'] = np.repeat(np.arange(0, len(pd_dataset_validation)/5000), 5000)\n",
    "\n",
    "    # Rearrange index\n",
    "    pd_dataset_train.reset_index(drop=True, inplace=True)\n",
    "    pd_dataset_test.reset_index(drop=True, inplace=True)\n",
    "    pd_dataset_validation.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save the data to a file\n",
    "    pd_dataset_train.to_csv(base_path + \"/data/pd_dataset_train.csv\", index=False)\n",
    "    pd_dataset_test.to_csv(base_path + \"/data/pd_dataset_test.csv\", index=False)\n",
    "    pd_dataset_validation.to_csv(base_path + \"/data/pd_dataset_validation.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HKA-EKG-Signalverarbeitung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

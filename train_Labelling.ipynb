{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "import ast\n",
    "\n",
    "import time\n",
    "\n",
    "import wfdb.processing\n",
    "import wfdb.processing.evaluate\n",
    "import wfdb.processing.qrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Methods for data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method defined by physionet to load data\n",
    "def load_raw_data(df, sampling_rate, path):\n",
    "    # Loading all data with signal and meta information\n",
    "    if sampling_rate == 100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    \n",
    "    # Eliminating meta information. We are selecting only signal value of 12 leads \n",
    "    data = np.array([signal for signal, meta in data])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Eine Datei kann nicht erstellt werden, wenn sie bereits vorhanden ist: 'C:\\\\\\\\Users\\\\\\\\Büro\\\\Documents\\\\\\\\Projekt_Lukas\\\\\\\\/data/pd_dataset_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m matlab_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBüro\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProjekt_Lukas\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMatlab_Labels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mrecords500\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/pd_dataset_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/pd_dataset_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(base_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/pd_dataset_val\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Eine Datei kann nicht erstellt werden, wenn sie bereits vorhanden ist: 'C:\\\\\\\\Users\\\\\\\\Büro\\\\Documents\\\\\\\\Projekt_Lukas\\\\\\\\/data/pd_dataset_train'"
     ]
    }
   ],
   "source": [
    "# base_path = r\"D:\\SynologyDrive\\10_Arbeit_und_Bildung\\20_Masterstudium\\01_Semester\\90_Projekt\\10_DEV\"\n",
    "base_path = r\"C:\\\\Users\\\\Büro\\Documents\\\\Projekt_Lukas\\\\\"\n",
    "path = base_path + \"/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/\"\n",
    "\n",
    "matlab_labels = r\"C:\\Users\\Büro\\Documents\\Projekt_Lukas\\Matlab_Labels\\records500\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloaded 0 of 12664 samples\n",
      "Preloaded 100 of 12664 samples\n",
      "Preloaded 200 of 12664 samples\n",
      "Preloaded 300 of 12664 samples\n",
      "Preloaded 400 of 12664 samples\n",
      "Preloaded 500 of 12664 samples\n",
      "Preloaded 600 of 12664 samples\n",
      "Preloaded 700 of 12664 samples\n",
      "Preloaded 800 of 12664 samples\n",
      "Preloaded 900 of 12664 samples\n",
      "Preloaded 1000 of 12664 samples\n",
      "Preloaded 1100 of 12664 samples\n",
      "Preloaded 1200 of 12664 samples\n",
      "Preloaded 1300 of 12664 samples\n",
      "Preloaded 1400 of 12664 samples\n",
      "Preloaded 1500 of 12664 samples\n",
      "Preloaded 1600 of 12664 samples\n",
      "Preloaded 1700 of 12664 samples\n",
      "Preloaded 1800 of 12664 samples\n",
      "Preloaded 1900 of 12664 samples\n",
      "Preloaded 2000 of 12664 samples\n",
      "Preloaded 2100 of 12664 samples\n",
      "Preloaded 2200 of 12664 samples\n",
      "Preloaded 2300 of 12664 samples\n",
      "Preloaded 2400 of 12664 samples\n",
      "Preloaded 2500 of 12664 samples\n",
      "Preloaded 2600 of 12664 samples\n",
      "Preloaded 2700 of 12664 samples\n",
      "Preloaded 2800 of 12664 samples\n",
      "Preloaded 2900 of 12664 samples\n",
      "Preloaded 3000 of 12664 samples\n",
      "Preloaded 3100 of 12664 samples\n",
      "Preloaded 3200 of 12664 samples\n",
      "Preloaded 3300 of 12664 samples\n",
      "Preloaded 3400 of 12664 samples\n",
      "Preloaded 3500 of 12664 samples\n",
      "Preloaded 3600 of 12664 samples\n",
      "Preloaded 3700 of 12664 samples\n",
      "Preloaded 3800 of 12664 samples\n",
      "Preloaded 3900 of 12664 samples\n",
      "Preloaded 4000 of 12664 samples\n",
      "Preloaded 4100 of 12664 samples\n",
      "Preloaded 4200 of 12664 samples\n",
      "Preloaded 4300 of 12664 samples\n",
      "Preloaded 4400 of 12664 samples\n",
      "Preloaded 4500 of 12664 samples\n",
      "Preloaded 4600 of 12664 samples\n",
      "Preloaded 4700 of 12664 samples\n",
      "Preloaded 4800 of 12664 samples\n",
      "Preloaded 4900 of 12664 samples\n",
      "Preloaded 5000 of 12664 samples\n",
      "Preloaded 5100 of 12664 samples\n",
      "Preloaded 5200 of 12664 samples\n",
      "Preloaded 5300 of 12664 samples\n",
      "Preloaded 5400 of 12664 samples\n",
      "Preloaded 5500 of 12664 samples\n",
      "Preloaded 5600 of 12664 samples\n",
      "Preloaded 5700 of 12664 samples\n",
      "Preloaded 5800 of 12664 samples\n",
      "Preloaded 5900 of 12664 samples\n",
      "Preloaded 6000 of 12664 samples\n",
      "Preloaded 6100 of 12664 samples\n",
      "Preloaded 6200 of 12664 samples\n",
      "Preloaded 6300 of 12664 samples\n",
      "Preloaded 6400 of 12664 samples\n",
      "Preloaded 6500 of 12664 samples\n",
      "Preloaded 6600 of 12664 samples\n",
      "Preloaded 6700 of 12664 samples\n",
      "Preloaded 6800 of 12664 samples\n",
      "Preloaded 6900 of 12664 samples\n",
      "Preloaded 7000 of 12664 samples\n",
      "Preloaded 7100 of 12664 samples\n",
      "Preloaded 7200 of 12664 samples\n",
      "Preloaded 7300 of 12664 samples\n",
      "Preloaded 7400 of 12664 samples\n",
      "Preloaded 7500 of 12664 samples\n",
      "Preloaded 7600 of 12664 samples\n",
      "Preloaded 7700 of 12664 samples\n",
      "Preloaded 7800 of 12664 samples\n",
      "Preloaded 7900 of 12664 samples\n",
      "Preloaded 8000 of 12664 samples\n",
      "Preloaded 8100 of 12664 samples\n",
      "Preloaded 8200 of 12664 samples\n",
      "Preloaded 8300 of 12664 samples\n",
      "Preloaded 8400 of 12664 samples\n",
      "Preloaded 8500 of 12664 samples\n",
      "Preloaded 8600 of 12664 samples\n",
      "Preloaded 8700 of 12664 samples\n",
      "Preloaded 8800 of 12664 samples\n",
      "Preloaded 8900 of 12664 samples\n",
      "Preloaded 9000 of 12664 samples\n",
      "Preloaded 9100 of 12664 samples\n",
      "Preloaded 9200 of 12664 samples\n",
      "Preloaded 9300 of 12664 samples\n",
      "Preloaded 9400 of 12664 samples\n",
      "Preloaded 9500 of 12664 samples\n",
      "Preloaded 9600 of 12664 samples\n",
      "Preloaded 9700 of 12664 samples\n",
      "Preloaded 9800 of 12664 samples\n",
      "Preloaded 9900 of 12664 samples\n",
      "Preloaded 10000 of 12664 samples\n",
      "Preloaded 10100 of 12664 samples\n",
      "Preloaded 10200 of 12664 samples\n",
      "Preloaded 10300 of 12664 samples\n",
      "Preloaded 10400 of 12664 samples\n",
      "Preloaded 10500 of 12664 samples\n",
      "Preloaded 10600 of 12664 samples\n",
      "Preloaded 10700 of 12664 samples\n",
      "Preloaded 10800 of 12664 samples\n",
      "Preloaded 10900 of 12664 samples\n",
      "Preloaded 11000 of 12664 samples\n",
      "Preloaded 11100 of 12664 samples\n",
      "Preloaded 11200 of 12664 samples\n",
      "Preloaded 11300 of 12664 samples\n",
      "Preloaded 11400 of 12664 samples\n",
      "Preloaded 11500 of 12664 samples\n",
      "Preloaded 11600 of 12664 samples\n",
      "Preloaded 11700 of 12664 samples\n",
      "Preloaded 11800 of 12664 samples\n",
      "Preloaded 11900 of 12664 samples\n",
      "Preloaded 12000 of 12664 samples\n",
      "Preloaded 12100 of 12664 samples\n",
      "Preloaded 12200 of 12664 samples\n",
      "Preloaded 12300 of 12664 samples\n",
      "Preloaded 12400 of 12664 samples\n",
      "Preloaded 12500 of 12664 samples\n",
      "Preloaded 12600 of 12664 samples\n",
      "Processed 0 of 12664 samples\n",
      "Processed 100 of 12664 samples\n",
      "Processed 200 of 12664 samples\n",
      "Processed 300 of 12664 samples\n",
      "Processed 400 of 12664 samples\n",
      "Processed 500 of 12664 samples\n",
      "Processed 600 of 12664 samples\n",
      "Processed 700 of 12664 samples\n",
      "Processed 800 of 12664 samples\n",
      "Processed 900 of 12664 samples\n",
      "Processed 1000 of 12664 samples\n",
      "Processed 1100 of 12664 samples\n",
      "Processed 1200 of 12664 samples\n",
      "Processed 1300 of 12664 samples\n",
      "Processed 1400 of 12664 samples\n",
      "Processed 1500 of 12664 samples\n",
      "Processed 1600 of 12664 samples\n",
      "Processed 1700 of 12664 samples\n",
      "Processed 1800 of 12664 samples\n",
      "Processed 1900 of 12664 samples\n",
      "Processed 2000 of 12664 samples\n",
      "Processed 2100 of 12664 samples\n",
      "Processed 2200 of 12664 samples\n",
      "Processed 2300 of 12664 samples\n",
      "Processed 2400 of 12664 samples\n",
      "Processed 2500 of 12664 samples\n",
      "Processed 2600 of 12664 samples\n",
      "Processed 2700 of 12664 samples\n",
      "Processed 2800 of 12664 samples\n",
      "Processed 2900 of 12664 samples\n",
      "Processed 3000 of 12664 samples\n",
      "Processed 3100 of 12664 samples\n",
      "Processed 3200 of 12664 samples\n",
      "Processed 3300 of 12664 samples\n",
      "Processed 3400 of 12664 samples\n",
      "Processed 3500 of 12664 samples\n",
      "Processed 3600 of 12664 samples\n",
      "Processed 3700 of 12664 samples\n",
      "Processed 3800 of 12664 samples\n",
      "Processed 3900 of 12664 samples\n",
      "Processed 4000 of 12664 samples\n",
      "Processed 4100 of 12664 samples\n",
      "Processed 4200 of 12664 samples\n",
      "Processed 4300 of 12664 samples\n",
      "Processed 4400 of 12664 samples\n",
      "Processed 4500 of 12664 samples\n",
      "Processed 4600 of 12664 samples\n",
      "Processed 4700 of 12664 samples\n",
      "Processed 4800 of 12664 samples\n",
      "Processed 4900 of 12664 samples\n",
      "Processed 5000 of 12664 samples\n",
      "Processed 5100 of 12664 samples\n",
      "Processed 5200 of 12664 samples\n",
      "Processed 5300 of 12664 samples\n",
      "Processed 5400 of 12664 samples\n",
      "Processed 5500 of 12664 samples\n",
      "Processed 5600 of 12664 samples\n",
      "Processed 5700 of 12664 samples\n",
      "Processed 5800 of 12664 samples\n",
      "Processed 5900 of 12664 samples\n",
      "Processed 6000 of 12664 samples\n",
      "Processed 6100 of 12664 samples\n",
      "Processed 6200 of 12664 samples\n",
      "Processed 6300 of 12664 samples\n",
      "Processed 6400 of 12664 samples\n",
      "Processed 6500 of 12664 samples\n",
      "Processed 6600 of 12664 samples\n",
      "Processed 6700 of 12664 samples\n",
      "Processed 6800 of 12664 samples\n",
      "Processed 6900 of 12664 samples\n",
      "Processed 7000 of 12664 samples\n",
      "Processed 7100 of 12664 samples\n",
      "Processed 7200 of 12664 samples\n",
      "Processed 7300 of 12664 samples\n",
      "Processed 7400 of 12664 samples\n",
      "Processed 7500 of 12664 samples\n",
      "Processed 7600 of 12664 samples\n",
      "Processed 7700 of 12664 samples\n",
      "Processed 7800 of 12664 samples\n",
      "Processed 7900 of 12664 samples\n",
      "Processed 8000 of 12664 samples\n",
      "Processed 8100 of 12664 samples\n",
      "Processed 8200 of 12664 samples\n",
      "Processed 8300 of 12664 samples\n",
      "Processed 8400 of 12664 samples\n",
      "Processed 8500 of 12664 samples\n",
      "Processed 8600 of 12664 samples\n",
      "Processed 8700 of 12664 samples\n",
      "Processed 8800 of 12664 samples\n",
      "Processed 8900 of 12664 samples\n",
      "Processed 9000 of 12664 samples\n",
      "Processed 9100 of 12664 samples\n",
      "Processed 9200 of 12664 samples\n",
      "Processed 9300 of 12664 samples\n",
      "Processed 9400 of 12664 samples\n",
      "Processed 9500 of 12664 samples\n",
      "Processed 9600 of 12664 samples\n",
      "Processed 9700 of 12664 samples\n",
      "Processed 9800 of 12664 samples\n",
      "Processed 9900 of 12664 samples\n",
      "Processed 10000 of 12664 samples\n",
      "Processed 10100 of 12664 samples\n",
      "Processed 10200 of 12664 samples\n",
      "Processed 10300 of 12664 samples\n",
      "Processed 10400 of 12664 samples\n",
      "Processed 10500 of 12664 samples\n",
      "Processed 10600 of 12664 samples\n",
      "Processed 10700 of 12664 samples\n",
      "Processed 10800 of 12664 samples\n",
      "Processed 10900 of 12664 samples\n",
      "Processed 11000 of 12664 samples\n",
      "Processed 11100 of 12664 samples\n",
      "Processed 11200 of 12664 samples\n",
      "Processed 11300 of 12664 samples\n",
      "Processed 11400 of 12664 samples\n",
      "Processed 11500 of 12664 samples\n",
      "Processed 11600 of 12664 samples\n",
      "Processed 11700 of 12664 samples\n",
      "Processed 11800 of 12664 samples\n",
      "Processed 11900 of 12664 samples\n",
      "Processed 12000 of 12664 samples\n",
      "Processed 12100 of 12664 samples\n",
      "Processed 12200 of 12664 samples\n",
      "Processed 12300 of 12664 samples\n",
      "Processed 12400 of 12664 samples\n",
      "Processed 12500 of 12664 samples\n",
      "Processed 12600 of 12664 samples\n"
     ]
    }
   ],
   "source": [
    "data_with_features_train = []\n",
    "data_with_features_test = []\n",
    "data_with_features_validation = []\n",
    "\n",
    "save_all = False\n",
    "enable_plot = False\t\n",
    "use_matlab_data = True\n",
    "\n",
    "import os\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0) #time.time_ns()%10000) # Set seed for reproducibility\n",
    "\n",
    "if use_matlab_data is False:\n",
    "    features_by_ecg_id = pd.read_csv(path+'ptbxl_database.csv', index_col='ecg_id')\n",
    "    features_by_ecg_id.scp_codes = features_by_ecg_id.scp_codes.apply(lambda x: ast.literal_eval(x))\n",
    "    array_length = len(features_by_ecg_id)\n",
    "\n",
    "if use_matlab_data is True:\n",
    "    files_str = os.listdir(matlab_labels)\n",
    "    array_length = len(files_str)\n",
    "\n",
    "    # Read all files and store in a list\n",
    "    features_by_ecg_id = []\n",
    "    for i in range(0, array_length):\n",
    "        features_by_ecg_id.append(pd.read_csv(matlab_labels + \"/\" + files_str[i]))\n",
    "\n",
    "        if (i % 100 == 0):\n",
    "            print(\"Preloaded \" + str(i) + \" of \" + str(array_length) + \" samples\")\n",
    "\n",
    "for i in range(0, array_length): # len(features_by_ecg_id)\n",
    "    \n",
    "    if use_matlab_data is False:\n",
    "\n",
    "        # Load raw data\n",
    "        features_by_ecg_id_selected = features_by_ecg_id.iloc[i:i+1]\n",
    "        raw_data_row_i = load_raw_data(features_by_ecg_id_selected, 500, path)[0]\n",
    "        \n",
    "        # Calculate the median lead of 12-lead-ecg\n",
    "        median_lead = np.transpose(np.median(np.transpose(raw_data_row_i), axis=0))\n",
    "        # Normalize median lead\n",
    "        median_lead = (median_lead - np.mean(median_lead)) / np.std(median_lead)\n",
    "\n",
    "        # Calculate the R-peaks\n",
    "        rpeaks = wfdb.processing.xqrs_detect(median_lead, fs=500, verbose=False)\n",
    "\n",
    "        # Generate feature vector and fill it with zeros, then fill it with 1 at the R-peak positions\n",
    "        feature_rpeak = np.zeros(len(median_lead))\n",
    "        feature_rpeak[rpeaks.astype(int)] = 1\n",
    "\n",
    "        # Generate time id (0-4999) for each sample\n",
    "        time_idx = np.arange(0, len(median_lead))\n",
    "        # Convert data type of time_idx to int\n",
    "        time_idx = time_idx.astype(int)\n",
    "        \n",
    "        # Build Pandas DataFrame containing raw data and features\n",
    "        df = pd.DataFrame({'time_idx': time_idx, 'raw_data': median_lead, 'feature_rpeak': feature_rpeak})\n",
    "\n",
    "    if use_matlab_data is True:\n",
    "\n",
    "        # Load raw data\n",
    "        raw_data_row_i = features_by_ecg_id[i]\n",
    "        raw_data_row_i = pd.DataFrame(raw_data_row_i)\n",
    "\n",
    "        # Features\n",
    "        feature_list = ['P-wave', 'P-peak', 'QRS-comples', 'R-peak', 'T-wave', 'T-peak']\n",
    "        \n",
    "        # Calculate the median lead of 12-lead-ecg\n",
    "        median_lead = raw_data_row_i['raw_data']\n",
    "        # Normalize median lead\n",
    "        median_lead = (median_lead - np.mean(median_lead)) / np.std(median_lead)\n",
    "        \n",
    "        df = raw_data_row_i\n",
    "        # Replace 'raw-data' with median\n",
    "        df['raw_data'] = median_lead\n",
    "\n",
    "    # Use random number to define if the data is used for training or testing or validation\n",
    "    random_number = np.random.rand()\n",
    "    if random_number < 0.7:\n",
    "        data_with_features_train.append(df)\n",
    "    elif random_number >= 0.7 and random_number < 0.85:\n",
    "        data_with_features_test.append(df)\n",
    "    else:\n",
    "        data_with_features_validation.append(df)\n",
    "\n",
    "    if( enable_plot ):\n",
    "        # Print with matplotlib\n",
    "        plt.plot(median_lead)\n",
    "        plt.plot(feature_rpeak)\n",
    "        # Make the plot larger\n",
    "        plt.gcf().set_size_inches(20, 10)\n",
    "        plt.show()\n",
    "\n",
    "    if (i % 100 == 0):\n",
    "        print(\"Processed \" + str(i) + \" of \" + str(array_length) + \" samples\")\n",
    "\n",
    "if not save_all:\n",
    "    # Delete folders and files\n",
    "    import shutil\n",
    "    shutil.rmtree(base_path + \"/data/pd_dataset_train\", ignore_errors=True)\n",
    "    shutil.rmtree(base_path + \"/data/pd_dataset_test\", ignore_errors=True)\n",
    "    shutil.rmtree(base_path + \"/data/pd_dataset_val\", ignore_errors=True)\n",
    "    # Generate Same structure again\n",
    "    import os\n",
    "    os.makedirs(base_path + \"/data/pd_dataset_train\")\n",
    "    os.makedirs(base_path + \"/data/pd_dataset_test\")\n",
    "    os.makedirs(base_path + \"/data/pd_dataset_val\")\n",
    "\n",
    "    # Iterate through all elements in data_with_features_train\n",
    "    # For each element, save 5 datasets with 512 datapoints\n",
    "    # start at datapoint 512 and end at len(data_with_features_train[i]) - 512, select the starting point randomly\n",
    "    for i in range(0, len(data_with_features_train)):\n",
    "        for j in range(0, 5):\n",
    "            # Select a random starting point\n",
    "            start_idx = np.random.randint(512, len(data_with_features_train[i]) - 512)\n",
    "            # Extract 512 datapoints\n",
    "            pd_dataset_train = data_with_features_train[i].iloc[start_idx:start_idx + 512]\n",
    "\n",
    "            # Save the data to a file\n",
    "            pd_dataset_train.to_csv(base_path + \"/data/pd_dataset_train/\" + str(i) + \"_\" + str(j) + \".csv\", index=False)\n",
    "\n",
    "    # Iterate through all elements in data_with_features_test\n",
    "    # For each element, save 5 datasets with 512 datapoints\n",
    "    # start at datapoint 512 and end at len(data_with_features_test[i]) - 512, select the starting point randomly\n",
    "    for i in range(0, len(data_with_features_test)):\n",
    "        for j in range(0, 5):\n",
    "            # Select a random starting point\n",
    "            start_idx = np.random.randint(512, len(data_with_features_test[i]) - 512)\n",
    "            # Extract 512 datapoints\n",
    "            pd_dataset_test = data_with_features_test[i].iloc[start_idx:start_idx + 512]\n",
    "\n",
    "            # Save the data to a file\n",
    "            pd_dataset_test.to_csv(base_path + \"/data/pd_dataset_test/\" + str(i) + \"_\" + str(j) + \".csv\", index=False)\n",
    "\n",
    "    # Iterate through all elements in data_with_features_validation\n",
    "    # For each element, save 5 datasets with 512 datapoints\n",
    "    # start at datapoint 512 and end at len(data_with_features_validation[i]) - 512, select the starting point randomly\n",
    "    for i in range(0, len(data_with_features_validation)):\n",
    "        for j in range(0, 5):\n",
    "            # Select a random starting point\n",
    "            start_idx = np.random.randint(512, len(data_with_features_validation[i]) - 512)\n",
    "            # Extract 512 datapoints\n",
    "            pd_dataset_validation = data_with_features_validation[i].iloc[start_idx:start_idx + 512]\n",
    "\n",
    "            # Save the data to a file\n",
    "            pd_dataset_validation.to_csv(base_path + \"/data/pd_dataset_val/\" + str(i) + \"_\" + str(j) + \".csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "if save_all:\n",
    "    # Convert List to DataFrame but segment it by using a column called group_id\n",
    "    pd_dataset_train = pd.concat(data_with_features_train)\n",
    "    pd_dataset_test = pd.concat(data_with_features_test)\n",
    "    pd_dataset_validation = pd.concat(data_with_features_validation)\n",
    "\n",
    "    # Delete variables that are not needed anymore\n",
    "    del data_with_features_train\n",
    "    del data_with_features_test\n",
    "    del data_with_features_validation\n",
    "\n",
    "    # Add a column to the DataFrame that segments the data into groups of 5000 samples\n",
    "    pd_dataset_train['group_ids'] = np.repeat(np.arange(0, len(pd_dataset_train)/5000), 5000)\n",
    "    pd_dataset_test['group_ids'] = np.repeat(np.arange(0, len(pd_dataset_test)/5000), 5000)\n",
    "    pd_dataset_validation['group_ids'] = np.repeat(np.arange(0, len(pd_dataset_validation)/5000), 5000)\n",
    "\n",
    "    # Rearrange index\n",
    "    pd_dataset_train.reset_index(drop=True, inplace=True)\n",
    "    pd_dataset_test.reset_index(drop=True, inplace=True)\n",
    "    pd_dataset_validation.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save the data to a file\n",
    "    pd_dataset_train.to_csv(base_path + \"/data/pd_dataset_train.csv\", index=False)\n",
    "    pd_dataset_test.to_csv(base_path + \"/data/pd_dataset_test.csv\", index=False)\n",
    "    pd_dataset_validation.to_csv(base_path + \"/data/pd_dataset_validation.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

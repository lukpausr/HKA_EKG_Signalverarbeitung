{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3fa61c3",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcdc5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from config.load_configuration import load_configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d352ac",
   "metadata": {},
   "source": [
    "#### Loading configuration\n",
    "\n",
    "This notebook loads configuration settings using the `load_configuration` function from the `config.load_configuration` module. The configuration is stored in the `config` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a036c93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC Name: DESKTOP-LUKAS\n",
      "Loaded configuration from ../config/config_lukas.yaml\n"
     ]
    }
   ],
   "source": [
    "config = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae862f8",
   "metadata": {},
   "source": [
    "#### Setting random ssed\n",
    "\n",
    "The random seed is set using `np.random.seed(config['seed'])` to ensure reproducibility of results throughout the data processing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1653cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abfae4d",
   "metadata": {},
   "source": [
    "#### Data Containers and Loading\n",
    "\n",
    "- `data_with_features_train`, `data_with_features_test`, and `data_with_features_validation` are lists used to store processed ECG data for training, testing, and validation.\n",
    "- All files from the directory specified in `config['path_to_matlab_data']` are loaded and read into `features_by_ecg_id` for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67111314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21754 files in the directory: C:\\Users\\lukas\\Documents\\HKA_DEV\\HKA_EKG_Signalverarbeitung_Data\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\\preprocessing_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading data...: 100%|██████████| 21754/21754 [07:48<00:00, 46.41file/s]\n"
     ]
    }
   ],
   "source": [
    "# Data containers\n",
    "data_with_features_train = []\n",
    "data_with_features_test = []\n",
    "data_with_features_validation = []\n",
    "\n",
    "# Load all files in the directory\n",
    "files = os.listdir(config['path_to_matlab_data'])\n",
    "print(\"Found \" + str(len(files)) + \" files in the directory: \" + config['path_to_matlab_data'])\n",
    "\n",
    "# Read all files and store in a list\n",
    "features_by_ecg_id = []\n",
    "for i in tqdm(range(len(files)), desc=\"Preloading data...\", unit=\"file\"):\n",
    "    features_by_ecg_id.append(pd.read_csv(config['path_to_matlab_data'] + \"/\" + files[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b752bc",
   "metadata": {},
   "source": [
    "#### Median Lead Calculation (disabled) and Normalization (disabled) and Test/Train/Val-Split\n",
    "\n",
    "For each ECG file, the median lead is extracted and normalized using either Z-score or Min-Max normalization, based on the configuration. The processed data is then randomly assigned to training, testing, or validation sets (70/15/15 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02731b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files by calculating the median lead and normalizing the data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 21754/21754 [00:00<00:00, 83674.99file/s]\n"
     ]
    }
   ],
   "source": [
    "def plot_raw_and_median_lead(raw_data_row_i, median_lead, i):\n",
    "    # Plot the raw data and the median data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(raw_data_row_i['raw_data'], label='Raw Data')\n",
    "    plt.plot(median_lead, label='Median Lead', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.title(f'ECG Data for Sample {i}')\n",
    "    plt.xlabel('Data Points')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Processing files by calculating the median lead and normalizing the data...\")\n",
    "for i in tqdm(range(0, len(files)), desc=\"Processing files\", unit=\"file\"):\n",
    "    # Load raw data\n",
    "    # raw_data_row_i = pd.DataFrame(features_by_ecg_id[i])\n",
    "    df = pd.DataFrame(features_by_ecg_id[i])\n",
    "\n",
    "    # Calculate the median lead of 12-lead-ecg\n",
    "    # median_lead = raw_data_row_i['raw_data']\n",
    "\n",
    "    # if config['normalization_method'] == \"z-score\":\n",
    "    #     # Normalize median lead using Z-score normalization\n",
    "    #     median_lead = (median_lead - np.mean(median_lead)) / np.std(median_lead)\n",
    "    # if config['normalization_method'] == \"min-max\":\n",
    "    #     # Normalize median lead using Min-Max normalization\n",
    "    #     median_lead = (median_lead - np.min(median_lead)) / (np.max(median_lead) - np.min(median_lead))\n",
    "\n",
    "    # plot_raw_and_median_lead(raw_data_row_i, median_lead, i)\n",
    "    \n",
    "    # Create a new DataFrame to store the data with features\n",
    "    # df = raw_data_row_i\n",
    "\n",
    "    # Replace 'raw-data' with median\n",
    "    # df['raw_data'] = median_lead\n",
    "\n",
    "    # Use random number to define if the data is used for training or testing or validation\n",
    "    # 70% of the data is used for training, 15% for testing and 15% for validation\n",
    "    random_number = np.random.rand()\n",
    "    if random_number < 0.7:\n",
    "        data_with_features_train.append(df)\n",
    "    elif random_number >= 0.7 and random_number < 0.85:\n",
    "        data_with_features_test.append(df)\n",
    "    else:\n",
    "        data_with_features_validation.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad5c4fb",
   "metadata": {},
   "source": [
    "#### Data Augmentation and Saving\n",
    "\n",
    "The processed ECG data is augmented by extracting multiple random 512-point segments from each sample in the training, testing, and validation sets. Each segment is saved as a separate CSV file in dedicated folders. Existing folders are cleared before saving the new augmented datasets. This step increases data diversity and prepares the data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccc94868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting and saving data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving train...: 100%|██████████| 15251/15251 [11:14<00:00, 22.60sample/s]\n",
      "Saving test...: 100%|██████████| 3219/3219 [02:27<00:00, 21.82sample/s]\n",
      "Saving validation...: 100%|██████████| 3284/3284 [02:29<00:00, 21.96sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Augmenting and saving data...\")\n",
    "\n",
    "perform_augmentation = False\n",
    "\n",
    "# Get paths to data folders\n",
    "path_train = config['path_to_data'] + \"/pd_dataset_train\"\n",
    "path_test = config['path_to_data'] + \"/pd_dataset_test\"\n",
    "path_val = config['path_to_data'] + \"/pd_dataset_val\"\n",
    "\n",
    "# Delete folders and files\n",
    "shutil.rmtree(path_train, ignore_errors=True)\n",
    "shutil.rmtree(path_test, ignore_errors=True)\n",
    "shutil.rmtree(path_val, ignore_errors=True)\n",
    "\n",
    "# Generate Same structure again\n",
    "os.makedirs(path_train)\n",
    "os.makedirs(path_test)\n",
    "os.makedirs(path_val)\n",
    "\n",
    "# Iterate through all elements in data_with_features_train\n",
    "# For each element, save 5 (parameter: augmentations) datasets with 512 datapoints\n",
    "# start at datapoint 512 and end at len(data_with_features_train[i]) - 512, select the starting point randomly\n",
    "if perform_augmentation:\n",
    "    for i in tqdm(range(0, len(data_with_features_train)), desc=\"Augmenting train...\", unit=\"sample\"):\n",
    "        for j in range(0, config['number_of_augmentations']):\n",
    "            # Select a random starting point\n",
    "            start_idx = np.random.randint(512, len(data_with_features_train[i]) - 512)\n",
    "            # Extract 512 datapoints\n",
    "            pd_dataset_train = data_with_features_train[i].iloc[start_idx:start_idx + 512]\n",
    "            # Save the data to a file\n",
    "            pd_dataset_train.to_csv(path_train + \"/\" + str(i) + \"_\" + str(j) + \".csv\", index=False)\n",
    "\n",
    "    for i in tqdm(range(0, len(data_with_features_test)), desc=\"Augmenting test...\", unit=\"sample\"):\n",
    "        for j in range(0, config['number_of_augmentations']):\n",
    "            start_idx = np.random.randint(512, len(data_with_features_test[i]) - 512)\n",
    "            pd_dataset_test = data_with_features_test[i].iloc[start_idx:start_idx + 512]\n",
    "            pd_dataset_test.to_csv(path_test + \"/\" + str(i) + \"_\" + str(j) + \".csv\", index=False)\n",
    "\n",
    "    for i in tqdm(range(0, len(data_with_features_validation)), desc=\"Augmenting validation...\", unit=\"sample\"):\n",
    "        for j in range(0, config['number_of_augmentations']):\n",
    "            start_idx = np.random.randint(512, len(data_with_features_validation[i]) - 512)\n",
    "            pd_dataset_validation = data_with_features_validation[i].iloc[start_idx:start_idx + 512]\n",
    "            pd_dataset_validation.to_csv(path_val + \"/\" + str(i) + \"_\" + str(j) + \".csv\", index=False)\n",
    "\n",
    "if not perform_augmentation: \n",
    "    for i in tqdm(range(0, len(data_with_features_train)), desc=\"Saving train...\", unit=\"sample\"):\n",
    "        data_with_features_train[i].to_csv(path_train + \"/\" + str(i) + \".csv\", index=False)\n",
    "\n",
    "    for i in tqdm(range(0, len(data_with_features_test)), desc=\"Saving test...\", unit=\"sample\"):\n",
    "        data_with_features_test[i].to_csv(path_test + \"/\" + str(i) + \".csv\", index=False)\n",
    "\n",
    "    for i in tqdm(range(0, len(data_with_features_validation)), desc=\"Saving validation...\", unit=\"sample\"):\n",
    "        data_with_features_validation[i].to_csv(path_val + \"/\" + str(i) + \".csv\", index=False)\n",
    "\n",
    "print(\"Data Preprocessing finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HKA_EKG_Signalverarbeitung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

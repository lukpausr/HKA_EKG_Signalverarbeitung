{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3fa61c3",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcdc5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "\n",
    "from config.load_configuration import load_configuration\n",
    "from data.datamodule import ECG_DataModule\n",
    "from model.model import UNET_1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d352ac",
   "metadata": {},
   "source": [
    "#### Loading configuration\n",
    "\n",
    "This notebook loads configuration settings using the `load_configuration` function from the `config.load_configuration` module. The configuration is stored in the `config` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a036c93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC Name: DESKTOP-LUKAS\n",
      "Loaded configuration from config/config_lukas.yaml\n"
     ]
    }
   ],
   "source": [
    "config = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef236e",
   "metadata": {},
   "source": [
    "#### Logging in to Weights & Biases (wandb)\n",
    "\n",
    "Before starting any experiment tracking, ensure you are logged in to your Weights & Biases (wandb) account. This enables automatic logging of metrics, model checkpoints, and experiment configurations. The following code logs you in to wandb:\n",
    "\n",
    "```python\n",
    "wandb.login()\n",
    "```\n",
    "If you are running this for the first time, you may be prompted to enter your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375929d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukas-pelz\u001b[0m (\u001b[33mHKA-EKG-Signalverarbeitung\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a2bf1",
   "metadata": {},
   "source": [
    "#### Setting Seeds for Reproducibility\n",
    "\n",
    "To ensure comparable and reproducible results, we set the random seed using the `seed_everything` function from PyTorch Lightning. This helps in achieving consistent behavior across multiple runs of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e672fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(config['seed'])\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"   # disable oneDNN optimizations for reproducibility\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e7fa5",
   "metadata": {},
   "source": [
    "#### Checking for GPU Devices\n",
    "\n",
    "In this step, we check for the availability of GPU devices and print the device currently being used by PyTorch. This ensures that the computations are performed on the most efficient hardware available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f0450c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Torch Version      : 2.7.0+cu128\n",
      "Selected Device    : cuda\n",
      "CUDA Version       : 12.8\n",
      "Device Name        : NVIDIA GeForce RTX 5060 Ti\n",
      "Memory Usage       : Allocated: 0.00 GB | Reserved: 0.00 GB\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"=\"*40)\n",
    "print(f\"Torch Version      : {torch.__version__}\")\n",
    "print(f\"Selected Device    : {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"CUDA Version       : {torch.version.cuda}\")\n",
    "    print(f\"Device Name        : {torch.cuda.get_device_name(0)}\")\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"Memory Usage       : Allocated: {allocated:.2f} GB | Reserved: {reserved:.2f} GB\")\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU.\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40cb2e8",
   "metadata": {},
   "source": [
    "#### Initializing the Data Module\n",
    "\n",
    "The `ECG_DataModule` is initialized using the data path, batch size, and feature list from the configuration. This prepares the data for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a53c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 4.7929e-02,  3.8235e-02,  2.8590e-02,  1.9708e-02,  1.2195e-02,\n",
      "          6.3819e-03,  2.1981e-03, -8.3775e-04, -3.4762e-03, -6.4977e-03,\n",
      "         -1.0486e-02, -1.5690e-02, -2.1992e-02, -2.8956e-02, -3.5935e-02,\n",
      "         -4.2190e-02, -4.7024e-02, -4.9905e-02, -5.0558e-02, -4.9023e-02,\n",
      "         -4.5645e-02, -4.1028e-02, -3.5963e-02, -3.1344e-02, -2.8020e-02,\n",
      "         -2.6617e-02, -2.7363e-02, -3.0024e-02, -3.3974e-02, -3.8402e-02,\n",
      "         -4.2604e-02, -4.6277e-02, -4.9659e-02, -5.3412e-02, -5.8275e-02,\n",
      "         -6.4655e-02, -7.2369e-02, -8.0631e-02, -8.8223e-02, -9.3761e-02,\n",
      "         -9.5921e-02, -9.3597e-02, -8.5904e-02, -7.2036e-02, -5.1030e-02,\n",
      "         -2.1583e-02,  1.7936e-02,  6.9222e-02,  1.3354e-01,  2.1113e-01,\n",
      "          3.0069e-01,  3.9899e-01,  5.0079e-01,  5.9897e-01,  6.8501e-01,\n",
      "          7.4992e-01,  7.8559e-01,  7.8630e-01,  7.5014e-01,  6.7966e-01,\n",
      "          5.8162e-01,  4.6583e-01,  3.4354e-01,  2.2580e-01,  1.2191e-01,\n",
      "          3.8272e-02, -2.2199e-02, -5.9975e-02, -7.8208e-02, -8.1659e-02,\n",
      "         -7.5553e-02, -6.4661e-02, -5.2705e-02, -4.2111e-02, -3.4076e-02,\n",
      "         -2.8864e-02, -2.6204e-02, -2.5606e-02, -2.6531e-02, -2.8459e-02,\n",
      "         -3.0938e-02, -3.3631e-02, -3.6328e-02, -3.8899e-02, -4.1206e-02,\n",
      "         -4.3048e-02, -4.4146e-02, -4.4206e-02, -4.3017e-02, -4.0549e-02,\n",
      "         -3.7003e-02, -3.2799e-02, -2.8523e-02, -2.4844e-02, -2.2398e-02,\n",
      "         -2.1625e-02, -2.2611e-02, -2.5025e-02, -2.8208e-02, -3.1371e-02,\n",
      "         -3.3811e-02, -3.5061e-02, -3.4981e-02, -3.3769e-02, -3.1884e-02,\n",
      "         -2.9869e-02, -2.8160e-02, -2.6953e-02, -2.6192e-02, -2.5629e-02,\n",
      "         -2.4893e-02, -2.3528e-02, -2.1057e-02, -1.7070e-02, -1.1349e-02,\n",
      "         -3.9964e-03,  4.4884e-03,  1.3241e-02,  2.1188e-02,  2.7271e-02,\n",
      "          3.0686e-02,  3.1059e-02,  2.8538e-02,  2.3766e-02,  1.7735e-02,\n",
      "          1.1553e-02,  6.1817e-03,  2.2366e-03, -1.0719e-04, -1.0824e-03,\n",
      "         -1.1882e-03, -9.9119e-04, -9.4280e-04, -1.2665e-03, -1.9316e-03,\n",
      "         -2.7073e-03, -3.2741e-03, -3.3551e-03, -2.8170e-03, -1.7042e-03,\n",
      "         -1.9395e-04,  1.4966e-03,  3.1956e-03,  4.8158e-03,  6.3367e-03,\n",
      "          7.7411e-03,  8.9474e-03,  9.8187e-03,  1.0286e-02,  1.0495e-02,\n",
      "          1.0828e-02,  1.1723e-02,  1.3440e-02,  1.5920e-02,  1.8827e-02,\n",
      "          2.1736e-02,  2.4351e-02,  2.6616e-02,  2.8662e-02,  3.0653e-02,\n",
      "          3.2668e-02,  3.4676e-02,  3.6562e-02,  3.8203e-02,  3.9538e-02,\n",
      "          4.0628e-02,  4.1669e-02,  4.2968e-02,  4.4881e-02,  4.7742e-02,\n",
      "          5.1779e-02,  5.7064e-02,  6.3502e-02,  7.0871e-02,  7.8912e-02,\n",
      "          8.7430e-02,  9.6324e-02,  1.0556e-01,  1.1508e-01,  1.2477e-01,\n",
      "          1.3443e-01,  1.4385e-01,  1.5280e-01,  1.6114e-01,  1.6872e-01,\n",
      "          1.7545e-01,  1.8129e-01,  1.8618e-01,  1.9009e-01,  1.9294e-01,\n",
      "          1.9460e-01,  1.9492e-01,  1.9390e-01,  1.9173e-01,  1.8888e-01,\n",
      "          1.8608e-01,  1.8421e-01,  1.8410e-01,  1.8633e-01,  1.9112e-01,\n",
      "          1.9821e-01,  2.0691e-01,  2.1629e-01,  2.2525e-01,  2.3272e-01,\n",
      "          2.3781e-01,  2.4000e-01,  2.3928e-01,  2.3617e-01,  2.3150e-01,\n",
      "          2.2619e-01,  2.2102e-01,  2.1654e-01,  2.1293e-01,  2.1007e-01,\n",
      "          2.0758e-01,  2.0493e-01,  2.0156e-01,  1.9695e-01,  1.9074e-01,\n",
      "          1.8274e-01,  1.7291e-01,  1.6135e-01,  1.4834e-01,  1.3428e-01,\n",
      "          1.1972e-01,  1.0519e-01,  9.1110e-02,  7.7709e-02,  6.4996e-02,\n",
      "          5.2814e-02,  4.0954e-02,  2.9283e-02,  1.7833e-02,  6.8154e-03,\n",
      "         -3.4485e-03, -1.2610e-02, -2.0345e-02, -2.6361e-02, -3.0404e-02,\n",
      "         -3.2309e-02, -3.2059e-02, -2.9860e-02, -2.6183e-02, -2.1738e-02,\n",
      "         -1.7360e-02, -1.3858e-02, -1.1883e-02, -1.1844e-02, -1.3858e-02,\n",
      "         -1.7713e-02, -2.2868e-02, -2.8534e-02, -3.3842e-02, -3.8045e-02,\n",
      "         -4.0644e-02, -4.1417e-02, -4.0355e-02, -3.7594e-02, -3.3367e-02,\n",
      "         -2.7992e-02, -2.1886e-02, -1.5569e-02, -9.6150e-03, -4.5369e-03,\n",
      "         -6.5921e-04,  1.9381e-03,  3.3803e-03,  3.9080e-03,  3.7901e-03,\n",
      "          3.2686e-03,  2.5359e-03,  1.7363e-03,  9.7870e-04,  3.4580e-04,\n",
      "         -1.0918e-04, -3.7408e-04, -4.8409e-04, -5.1391e-04, -5.5742e-04,\n",
      "         -7.0293e-04, -1.0144e-03, -1.5253e-03, -2.2402e-03, -3.1355e-03,\n",
      "         -4.1548e-03, -5.2054e-03, -6.1628e-03, -6.8867e-03, -7.2427e-03,\n",
      "         -7.1248e-03, -6.4771e-03, -5.3187e-03, -3.7658e-03, -2.0448e-03,\n",
      "         -4.8852e-04,  4.9485e-04,  4.8785e-04, -8.3825e-04, -3.6195e-03,\n",
      "         -7.7272e-03, -1.2759e-02, -1.8097e-02, -2.3007e-02, -2.6760e-02,\n",
      "         -2.8760e-02, -2.8658e-02, -2.6428e-02, -2.2367e-02, -1.7014e-02,\n",
      "         -1.1037e-02, -5.1059e-03,  2.0205e-04,  4.4951e-03,  7.6236e-03,\n",
      "          9.6959e-03,  1.1036e-02,  1.2093e-02,  1.3327e-02,  1.5098e-02,\n",
      "          1.7573e-02,  2.0676e-02,  2.4117e-02,  2.7494e-02,  3.0433e-02,\n",
      "          3.2691e-02,  3.4173e-02,  3.4918e-02,  3.5054e-02,  3.4774e-02,\n",
      "          3.4288e-02,  3.3784e-02,  3.3371e-02,  3.3063e-02,  3.2785e-02,\n",
      "          3.2416e-02,  3.1827e-02,  3.0913e-02,  2.9615e-02,  2.7953e-02,\n",
      "          2.6048e-02,  2.4138e-02,  2.2542e-02,  2.1580e-02,  2.1446e-02,\n",
      "          2.2100e-02,  2.3210e-02,  2.4215e-02,  2.4453e-02,  2.3343e-02,\n",
      "          2.0568e-02,  1.6204e-02,  1.0737e-02,  4.9170e-03, -4.9276e-04,\n",
      "         -4.9434e-03, -8.2024e-03, -1.0320e-02, -1.1520e-02, -1.2089e-02,\n",
      "         -1.2284e-02, -1.2288e-02, -1.2210e-02, -1.2119e-02, -1.2081e-02,\n",
      "         -1.2174e-02, -1.2472e-02, -1.3010e-02, -1.3760e-02, -1.4623e-02,\n",
      "         -1.5461e-02, -1.6140e-02, -1.6595e-02, -1.6851e-02, -1.7018e-02,\n",
      "         -1.7263e-02, -1.7750e-02, -1.8590e-02, -1.9782e-02, -2.1171e-02,\n",
      "         -2.2427e-02, -2.3068e-02, -2.2550e-02, -2.0384e-02, -1.6239e-02,\n",
      "         -1.0008e-02, -1.8347e-03,  7.8441e-03,  1.8262e-02,  2.8316e-02,\n",
      "          3.6666e-02,  4.1995e-02,  4.3355e-02,  4.0473e-02,  3.3842e-02,\n",
      "          2.4582e-02,  1.4136e-02,  3.9296e-03, -4.9103e-03, -1.1712e-02,\n",
      "         -1.6273e-02, -1.8759e-02, -1.9569e-02, -1.9213e-02, -1.8229e-02,\n",
      "         -1.7118e-02, -1.6276e-02, -1.5920e-02, -1.6041e-02, -1.6394e-02,\n",
      "         -1.6564e-02, -1.6075e-02, -1.4531e-02, -1.1722e-02, -7.6794e-03,\n",
      "         -2.6757e-03,  2.8272e-03,  8.2457e-03,  1.2956e-02,  1.6387e-02,\n",
      "          1.8114e-02,  1.7966e-02,  1.6071e-02,  1.2848e-02,  8.8853e-03,\n",
      "          4.7843e-03,  1.0178e-03, -2.1246e-03, -4.5108e-03, -6.1117e-03,\n",
      "         -6.9670e-03, -7.1680e-03, -6.8202e-03, -5.9769e-03, -4.5885e-03,\n",
      "         -2.5117e-03,  4.4432e-04,  4.4506e-03,  9.5461e-03,  1.5488e-02,\n",
      "          2.1680e-02,  2.7303e-02,  3.1607e-02,  3.4216e-02,  3.5262e-02,\n",
      "          3.5317e-02,  3.5175e-02,  3.5580e-02,  3.7005e-02,  3.9597e-02,\n",
      "          4.3252e-02,  4.7750e-02,  5.2872e-02,  5.8449e-02,  6.4356e-02,\n",
      "          7.0476e-02,  7.6663e-02,  8.2729e-02,  8.8445e-02,  9.3563e-02,\n",
      "          9.7823e-02,  1.0097e-01,  1.0277e-01,  1.0303e-01,  1.0160e-01,\n",
      "          9.8425e-02,  9.3536e-02,  8.7053e-02,  7.9199e-02,  7.0273e-02,\n",
      "          6.0587e-02,  5.0407e-02,  3.9973e-02,  2.9575e-02,  1.9587e-02,\n",
      "          1.0388e-02,  2.2488e-03, -4.7286e-03, -1.0586e-02, -1.5444e-02,\n",
      "         -1.9434e-02, -2.2642e-02, -2.5100e-02, -2.6822e-02, -2.7865e-02,\n",
      "         -2.8391e-02, -2.8680e-02, -2.9080e-02, -2.9871e-02, -3.1105e-02,\n",
      "         -3.2496e-02, -3.3488e-02, -3.3469e-02, -3.2007e-02, -2.8994e-02,\n",
      "         -2.4643e-02, -1.9419e-02, -1.3954e-02, -8.9662e-03, -5.1967e-03,\n",
      "         -3.3330e-03, -3.9220e-03, -7.2713e-03, -1.3364e-02, -2.1803e-02,\n",
      "         -3.1814e-02, -4.2285e-02, -5.1850e-02, -5.8916e-02, -6.1631e-02,\n",
      "         -5.7799e-02, -4.4872e-02]]), tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "dm = ECG_DataModule(\n",
    "    data_dir=config['path_to_data'],\n",
    "    batch_size=config['batch_size'],\n",
    "    # num_workers=0,\n",
    "    # persistent_workers=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    persistent_workers=config['persistent_workers'],\n",
    "    feature_list=config['feature_list']\n",
    ")\n",
    "dm.setup()\n",
    "dm.train_dataset.__getitem__(0)  # Warm up dataset (for reproducibility when using num_workers > 0)\n",
    "print(dm.train_dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0693029f",
   "metadata": {},
   "source": [
    "#### Creating the Model\n",
    "\n",
    "In this step, we will define the model architecture and print its summary using the `ModelSummary` utility from PyTorch Lightning. This provides an overview of the model's layers, parameters, and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038f7c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | Name                                            | Type                  | Params | Mode  | In sizes      | Out sizes    \n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "0   | criterion                                       | BCEWithLogitsLoss     | 0      | train | ?             | ?            \n",
      "1   | train_jaccard                                   | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "2   | val_jaccard                                     | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "3   | test_jaccard                                    | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "4   | multi_tolerance_metrics                         | MultiToleranceWrapper | 0      | train | ?             | ?            \n",
      "5   | multi_tolerance_metrics.metrics                 | ModuleDict            | 0      | train | ?             | ?            \n",
      "6   | multi_tolerance_metrics.metrics.tolerance_5ms   | CustomMetrics         | 0      | train | ?             | ?            \n",
      "7   | multi_tolerance_metrics.metrics.tolerance_10ms  | CustomMetrics         | 0      | train | ?             | ?            \n",
      "8   | multi_tolerance_metrics.metrics.tolerance_40ms  | CustomMetrics         | 0      | train | ?             | ?            \n",
      "9   | multi_tolerance_metrics.metrics.tolerance_150ms | CustomMetrics         | 0      | train | ?             | ?            \n",
      "10  | AvgPool1D1                                      | AvgPool1d             | 0      | train | [1, 64, 512]  | [1, 64, 256] \n",
      "11  | AvgPool1D2                                      | AvgPool1d             | 0      | train | [1, 128, 256] | [1, 128, 128]\n",
      "12  | AvgPool1D3                                      | AvgPool1d             | 0      | train | [1, 256, 128] | [1, 256, 64] \n",
      "13  | AvgPool1D4                                      | AvgPool1d             | 0      | train | [1, 512, 64]  | [1, 512, 32] \n",
      "14  | layer1                                          | Sequential            | 21.2 K | train | [1, 1, 512]   | [1, 64, 512] \n",
      "15  | layer1.0                                        | conbr_block           | 512    | train | [1, 1, 512]   | [1, 64, 512] \n",
      "16  | layer1.0.net                                    | Sequential            | 512    | train | [1, 1, 512]   | [1, 64, 512] \n",
      "17  | layer1.0.net.0                                  | Conv1d                | 384    | train | [1, 1, 512]   | [1, 64, 512] \n",
      "18  | layer1.0.net.1                                  | ReLU                  | 0      | train | [1, 64, 512]  | [1, 64, 512] \n",
      "19  | layer1.0.net.2                                  | BatchNorm1d           | 128    | train | [1, 64, 512]  | [1, 64, 512] \n",
      "20  | layer1.1                                        | conbr_block           | 20.7 K | train | [1, 64, 512]  | [1, 64, 512] \n",
      "21  | layer1.1.net                                    | Sequential            | 20.7 K | train | [1, 64, 512]  | [1, 64, 512] \n",
      "22  | layer1.1.net.0                                  | Conv1d                | 20.5 K | train | [1, 64, 512]  | [1, 64, 512] \n",
      "23  | layer1.1.net.1                                  | ReLU                  | 0      | train | [1, 64, 512]  | [1, 64, 512] \n",
      "24  | layer1.1.net.2                                  | BatchNorm1d           | 128    | train | [1, 64, 512]  | [1, 64, 512] \n",
      "25  | layer2                                          | Sequential            | 123 K  | train | [1, 64, 256]  | [1, 128, 256]\n",
      "26  | layer2.0                                        | conbr_block           | 41.3 K | train | [1, 64, 256]  | [1, 128, 256]\n",
      "27  | layer2.0.net                                    | Sequential            | 41.3 K | train | [1, 64, 256]  | [1, 128, 256]\n",
      "28  | layer2.0.net.0                                  | Conv1d                | 41.1 K | train | [1, 64, 256]  | [1, 128, 256]\n",
      "29  | layer2.0.net.1                                  | ReLU                  | 0      | train | [1, 128, 256] | [1, 128, 256]\n",
      "30  | layer2.0.net.2                                  | BatchNorm1d           | 256    | train | [1, 128, 256] | [1, 128, 256]\n",
      "31  | layer2.1                                        | conbr_block           | 82.3 K | train | [1, 128, 256] | [1, 128, 256]\n",
      "32  | layer2.1.net                                    | Sequential            | 82.3 K | train | [1, 128, 256] | [1, 128, 256]\n",
      "33  | layer2.1.net.0                                  | Conv1d                | 82.0 K | train | [1, 128, 256] | [1, 128, 256]\n",
      "34  | layer2.1.net.1                                  | ReLU                  | 0      | train | [1, 128, 256] | [1, 128, 256]\n",
      "35  | layer2.1.net.2                                  | BatchNorm1d           | 256    | train | [1, 128, 256] | [1, 128, 256]\n",
      "36  | layer3                                          | Sequential            | 493 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "37  | layer3.0                                        | conbr_block           | 164 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "38  | layer3.0.net                                    | Sequential            | 164 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "39  | layer3.0.net.0                                  | Conv1d                | 164 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "40  | layer3.0.net.1                                  | ReLU                  | 0      | train | [1, 256, 128] | [1, 256, 128]\n",
      "41  | layer3.0.net.2                                  | BatchNorm1d           | 512    | train | [1, 256, 128] | [1, 256, 128]\n",
      "42  | layer3.1                                        | conbr_block           | 328 K  | train | [1, 256, 128] | [1, 256, 128]\n",
      "43  | layer3.1.net                                    | Sequential            | 328 K  | train | [1, 256, 128] | [1, 256, 128]\n",
      "44  | layer3.1.net.0                                  | Conv1d                | 327 K  | train | [1, 256, 128] | [1, 256, 128]\n",
      "45  | layer3.1.net.1                                  | ReLU                  | 0      | train | [1, 256, 128] | [1, 256, 128]\n",
      "46  | layer3.1.net.2                                  | BatchNorm1d           | 512    | train | [1, 256, 128] | [1, 256, 128]\n",
      "47  | layer4                                          | Sequential            | 2.0 M  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "48  | layer4.0                                        | conbr_block           | 656 K  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "49  | layer4.0.net                                    | Sequential            | 656 K  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "50  | layer4.0.net.0                                  | Conv1d                | 655 K  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "51  | layer4.0.net.1                                  | ReLU                  | 0      | train | [1, 512, 64]  | [1, 512, 64] \n",
      "52  | layer4.0.net.2                                  | BatchNorm1d           | 1.0 K  | train | [1, 512, 64]  | [1, 512, 64] \n",
      "53  | layer4.1                                        | conbr_block           | 1.3 M  | train | [1, 512, 64]  | [1, 512, 64] \n",
      "54  | layer4.1.net                                    | Sequential            | 1.3 M  | train | [1, 512, 64]  | [1, 512, 64] \n",
      "55  | layer4.1.net.0                                  | Conv1d                | 1.3 M  | train | [1, 512, 64]  | [1, 512, 64] \n",
      "56  | layer4.1.net.1                                  | ReLU                  | 0      | train | [1, 512, 64]  | [1, 512, 64] \n",
      "57  | layer4.1.net.2                                  | BatchNorm1d           | 1.0 K  | train | [1, 512, 64]  | [1, 512, 64] \n",
      "58  | layer5                                          | Sequential            | 7.9 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "59  | layer5.0                                        | conbr_block           | 2.6 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "60  | layer5.0.net                                    | Sequential            | 2.6 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "61  | layer5.0.net.0                                  | Conv1d                | 2.6 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "62  | layer5.0.net.1                                  | ReLU                  | 0      | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "63  | layer5.0.net.2                                  | BatchNorm1d           | 2.0 K  | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "64  | layer5.1                                        | conbr_block           | 5.2 M  | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "65  | layer5.1.net                                    | Sequential            | 5.2 M  | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "66  | layer5.1.net.0                                  | Conv1d                | 5.2 M  | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "67  | layer5.1.net.1                                  | ReLU                  | 0      | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "68  | layer5.1.net.2                                  | BatchNorm1d           | 2.0 K  | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "69  | layer5T                                         | Sequential            | 2.6 M  | train | [1, 1024, 32] | [1, 512, 64] \n",
      "70  | layer5T.0                                       | ConvTranspose1d       | 2.6 M  | train | [1, 1024, 32] | [1, 512, 64] \n",
      "71  | layer4T                                         | Sequential            | 3.6 M  | train | [1, 1024, 64] | [1, 256, 128]\n",
      "72  | layer4T.0                                       | conbr_block           | 2.6 M  | train | [1, 1024, 64] | [1, 512, 64] \n",
      "73  | layer4T.0.net                                   | Sequential            | 2.6 M  | train | [1, 1024, 64] | [1, 512, 64] \n",
      "74  | layer4T.0.net.0                                 | Conv1d                | 2.6 M  | train | [1, 1024, 64] | [1, 512, 64] \n",
      "75  | layer4T.0.net.1                                 | ReLU                  | 0      | train | [1, 512, 64]  | [1, 512, 64] \n",
      "76  | layer4T.0.net.2                                 | BatchNorm1d           | 1.0 K  | train | [1, 512, 64]  | [1, 512, 64] \n",
      "77  | layer4T.1                                       | conbr_block           | 656 K  | train | [1, 512, 64]  | [1, 256, 64] \n",
      "78  | layer4T.1.net                                   | Sequential            | 656 K  | train | [1, 512, 64]  | [1, 256, 64] \n",
      "79  | layer4T.1.net.0                                 | Conv1d                | 655 K  | train | [1, 512, 64]  | [1, 256, 64] \n",
      "80  | layer4T.1.net.1                                 | ReLU                  | 0      | train | [1, 256, 64]  | [1, 256, 64] \n",
      "81  | layer4T.1.net.2                                 | BatchNorm1d           | 512    | train | [1, 256, 64]  | [1, 256, 64] \n",
      "82  | layer4T.2                                       | ConvTranspose1d       | 327 K  | train | [1, 256, 64]  | [1, 256, 128]\n",
      "83  | layer3T                                         | Sequential            | 902 K  | train | [1, 512, 128] | [1, 128, 256]\n",
      "84  | layer3T.0                                       | conbr_block           | 656 K  | train | [1, 512, 128] | [1, 256, 128]\n",
      "85  | layer3T.0.net                                   | Sequential            | 656 K  | train | [1, 512, 128] | [1, 256, 128]\n",
      "86  | layer3T.0.net.0                                 | Conv1d                | 655 K  | train | [1, 512, 128] | [1, 256, 128]\n",
      "87  | layer3T.0.net.1                                 | ReLU                  | 0      | train | [1, 256, 128] | [1, 256, 128]\n",
      "88  | layer3T.0.net.2                                 | BatchNorm1d           | 512    | train | [1, 256, 128] | [1, 256, 128]\n",
      "89  | layer3T.1                                       | conbr_block           | 164 K  | train | [1, 256, 128] | [1, 128, 128]\n",
      "90  | layer3T.1.net                                   | Sequential            | 164 K  | train | [1, 256, 128] | [1, 128, 128]\n",
      "91  | layer3T.1.net.0                                 | Conv1d                | 163 K  | train | [1, 256, 128] | [1, 128, 128]\n",
      "92  | layer3T.1.net.1                                 | ReLU                  | 0      | train | [1, 128, 128] | [1, 128, 128]\n",
      "93  | layer3T.1.net.2                                 | BatchNorm1d           | 256    | train | [1, 128, 128] | [1, 128, 128]\n",
      "94  | layer3T.2                                       | ConvTranspose1d       | 82.0 K | train | [1, 128, 128] | [1, 128, 256]\n",
      "95  | layer2T                                         | Sequential            | 225 K  | train | [1, 256, 256] | [1, 64, 512] \n",
      "96  | layer2T.0                                       | conbr_block           | 164 K  | train | [1, 256, 256] | [1, 128, 256]\n",
      "97  | layer2T.0.net                                   | Sequential            | 164 K  | train | [1, 256, 256] | [1, 128, 256]\n",
      "98  | layer2T.0.net.0                                 | Conv1d                | 163 K  | train | [1, 256, 256] | [1, 128, 256]\n",
      "99  | layer2T.0.net.1                                 | ReLU                  | 0      | train | [1, 128, 256] | [1, 128, 256]\n",
      "100 | layer2T.0.net.2                                 | BatchNorm1d           | 256    | train | [1, 128, 256] | [1, 128, 256]\n",
      "101 | layer2T.1                                       | conbr_block           | 41.2 K | train | [1, 128, 256] | [1, 64, 256] \n",
      "102 | layer2T.1.net                                   | Sequential            | 41.2 K | train | [1, 128, 256] | [1, 64, 256] \n",
      "103 | layer2T.1.net.0                                 | Conv1d                | 41.0 K | train | [1, 128, 256] | [1, 64, 256] \n",
      "104 | layer2T.1.net.1                                 | ReLU                  | 0      | train | [1, 64, 256]  | [1, 64, 256] \n",
      "105 | layer2T.1.net.2                                 | BatchNorm1d           | 128    | train | [1, 64, 256]  | [1, 64, 256] \n",
      "106 | layer2T.2                                       | ConvTranspose1d       | 20.5 K | train | [1, 64, 256]  | [1, 64, 512] \n",
      "107 | layer1Out                                       | Sequential            | 43.1 K | train | [1, 128, 512] | [1, 6, 512]  \n",
      "108 | layer1Out.0                                     | conbr_block           | 41.2 K | train | [1, 128, 512] | [1, 64, 512] \n",
      "109 | layer1Out.0.net                                 | Sequential            | 41.2 K | train | [1, 128, 512] | [1, 64, 512] \n",
      "110 | layer1Out.0.net.0                               | Conv1d                | 41.0 K | train | [1, 128, 512] | [1, 64, 512] \n",
      "111 | layer1Out.0.net.1                               | ReLU                  | 0      | train | [1, 64, 512]  | [1, 64, 512] \n",
      "112 | layer1Out.0.net.2                               | BatchNorm1d           | 128    | train | [1, 64, 512]  | [1, 64, 512] \n",
      "113 | layer1Out.1                                     | conbr_block           | 1.9 K  | train | [1, 64, 512]  | [1, 6, 512]  \n",
      "114 | layer1Out.1.net                                 | Sequential            | 1.9 K  | train | [1, 64, 512]  | [1, 6, 512]  \n",
      "115 | layer1Out.1.net.0                               | Conv1d                | 1.9 K  | train | [1, 64, 512]  | [1, 6, 512]  \n",
      "116 | layer1Out.1.net.1                               | ReLU                  | 0      | train | [1, 6, 512]   | [1, 6, 512]  \n",
      "117 | layer1Out.1.net.2                               | BatchNorm1d           | 12     | train | [1, 6, 512]   | [1, 6, 512]  \n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "17.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.9 M    Total params\n",
      "71.512    Total estimated model params size (MB)\n",
      "118       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "UNET_1D\n"
     ]
    }
   ],
   "source": [
    "model = UNET_1D(\n",
    "    in_channels=1, \n",
    "    layer_n=512, \n",
    "    out_channels=len(config['feature_list']), \n",
    "    kernel_size=5\n",
    ")\n",
    "print(ModelSummary(model, max_depth=-1))  \n",
    "print(type(model).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000edf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time       : 2025-09-09_12-41\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (2.6s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250909_124111-cd8616db</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/cd8616db' target=\"_blank\">manual_training_1_lead_UNET_1D_2025-09-09_12-41</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/cd8616db' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/cd8616db</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                    | Type                  | Params | Mode  | In sizes      | Out sizes    \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "0  | criterion               | BCEWithLogitsLoss     | 0      | train | ?             | ?            \n",
      "1  | train_jaccard           | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "2  | val_jaccard             | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "3  | test_jaccard            | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "4  | multi_tolerance_metrics | MultiToleranceWrapper | 0      | train | ?             | ?            \n",
      "5  | AvgPool1D1              | AvgPool1d             | 0      | train | [1, 64, 512]  | [1, 64, 256] \n",
      "6  | AvgPool1D2              | AvgPool1d             | 0      | train | [1, 128, 256] | [1, 128, 128]\n",
      "7  | AvgPool1D3              | AvgPool1d             | 0      | train | [1, 256, 128] | [1, 256, 64] \n",
      "8  | AvgPool1D4              | AvgPool1d             | 0      | train | [1, 512, 64]  | [1, 512, 32] \n",
      "9  | layer1                  | Sequential            | 21.2 K | train | [1, 1, 512]   | [1, 64, 512] \n",
      "10 | layer2                  | Sequential            | 123 K  | train | [1, 64, 256]  | [1, 128, 256]\n",
      "11 | layer3                  | Sequential            | 493 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "12 | layer4                  | Sequential            | 2.0 M  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "13 | layer5                  | Sequential            | 7.9 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "14 | layer5T                 | Sequential            | 2.6 M  | train | [1, 1024, 32] | [1, 512, 64] \n",
      "15 | layer4T                 | Sequential            | 3.6 M  | train | [1, 1024, 64] | [1, 256, 128]\n",
      "16 | layer3T                 | Sequential            | 902 K  | train | [1, 512, 128] | [1, 128, 256]\n",
      "17 | layer2T                 | Sequential            | 225 K  | train | [1, 256, 256] | [1, 64, 512] \n",
      "18 | layer1Out               | Sequential            | 43.1 K | train | [1, 128, 512] | [1, 6, 512]  \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "17.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.9 M    Total params\n",
      "71.512    Total estimated model params size (MB)\n",
      "118       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  29%|██▊       | 137/477 [00:06<00:15, 21.75it/s, v_num=16db, train_loss_step=0.476]"
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "print(f\"Current Time       : {current_time}\")\n",
    "\n",
    "# Initialize wandb logger (https://wandb.ai/HKA-EKG-Signalverarbeitung)\n",
    "wandb_logger = WandbLogger(\n",
    "    project=config['wandb_project_name'],\n",
    "    name=f\"{config['wandb_experiment_name']}_{type(model).__name__}_{current_time}\",\n",
    "    config={\n",
    "        'model': type(model).__name__,\n",
    "        'dataset': type(dm).__name__,\n",
    "        'batch_size': config['batch_size'],\n",
    "        'max_epochs': config['max_epochs'],\n",
    "        'learning_rate': config['learning_rate']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize Trainer with wandb logger, using early stopping callback (https://lightning.ai/docs/pytorch/stable/common/early_stopping.html)\n",
    "trainer = Trainer(\n",
    "    max_epochs=config['max_epochs'], \n",
    "    default_root_dir='model/checkpoint/', #data_directory, \n",
    "    accelerator=\"auto\", \n",
    "    devices=\"auto\", \n",
    "    strategy=\"auto\",\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')], \n",
    "    logger=wandb_logger)\n",
    "\n",
    "trainer.fit(model=model, datamodule=dm)\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish()\n",
    "\n",
    "# Create a filename with date identifier\n",
    "model_filename = f\"{config['wandb_experiment_name']}_{type(model).__name__}_{current_time}.ckpt\"\n",
    "\n",
    "# Save the model's state_dict to the path specified in config\n",
    "save_path = os.path.join(config['path_to_models'], model_filename)\n",
    "trainer.save_checkpoint(save_path)\n",
    "print(f\"Model checkpoint saved as {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e144e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\HKA_EKG_Signalverarbeitung\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-09-04 10:42:04,031] A new study created in memory with name: Optuna_HPO_2025-09-04_10-42\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\lukas\\anaconda3\\envs\\HKA_EKG_Signalverarbeitung\\Lib\\site-packages\\optuna\\distributions.py:699: UserWarning: The distribution is specified by [3, 5] and step=10, but the range is not divisible by `step`. It will be replaced by [3, 3].\n",
      "  warnings.warn(\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250904_104204-f1jv7cr7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/f1jv7cr7' target=\"_blank\">type_bs64_lr3.1e-04_wd2.7e-04_optAdamW_schReduceLROnPlateau_acc1_2025-09-04_10-42</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/f1jv7cr7' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/f1jv7cr7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                    | Type                  | Params | Mode  | In sizes      | Out sizes    \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "0  | criterion               | BCEWithLogitsLoss     | 0      | train | ?             | ?            \n",
      "1  | train_jaccard           | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "2  | val_jaccard             | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "3  | test_jaccard            | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "4  | multi_tolerance_metrics | MultiToleranceWrapper | 0      | train | ?             | ?            \n",
      "5  | AvgPool1D1              | AvgPool1d             | 0      | train | [1, 64, 512]  | [1, 64, 256] \n",
      "6  | AvgPool1D2              | AvgPool1d             | 0      | train | [1, 128, 256] | [1, 128, 128]\n",
      "7  | AvgPool1D3              | AvgPool1d             | 0      | train | [1, 256, 128] | [1, 256, 64] \n",
      "8  | AvgPool1D4              | AvgPool1d             | 0      | train | [1, 512, 64]  | [1, 512, 32] \n",
      "9  | layer1                  | Sequential            | 21.2 K | train | [1, 1, 512]   | [1, 64, 512] \n",
      "10 | layer2                  | Sequential            | 123 K  | train | [1, 64, 256]  | [1, 128, 256]\n",
      "11 | layer3                  | Sequential            | 493 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "12 | layer4                  | Sequential            | 2.0 M  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "13 | layer5                  | Sequential            | 7.9 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "14 | layer5T                 | Sequential            | 2.6 M  | train | [1, 1024, 32] | [1, 512, 64] \n",
      "15 | layer4T                 | Sequential            | 3.6 M  | train | [1, 1024, 64] | [1, 256, 128]\n",
      "16 | layer3T                 | Sequential            | 902 K  | train | [1, 512, 128] | [1, 128, 256]\n",
      "17 | layer2T                 | Sequential            | 225 K  | train | [1, 256, 256] | [1, 64, 512] \n",
      "18 | layer1Out               | Sequential            | 43.1 K | train | [1, 128, 512] | [1, 6, 512]  \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "17.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.9 M    Total params\n",
      "71.512    Total estimated model params size (MB)\n",
      "118       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 840/840 [00:44<00:00, 19.01it/s, v_num=7cr7, train_loss_step=0.272, val_loss_step=0.270, val_loss_epoch=0.282, train_loss_epoch=0.307]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 840/840 [00:44<00:00, 18.70it/s, v_num=7cr7, train_loss_step=0.272, val_loss_step=0.270, val_loss_epoch=0.282, train_loss_epoch=0.307]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅███████████</td></tr><tr><td>train_loss_epoch</td><td>█▄▁</td></tr><tr><td>train_loss_step</td><td>██▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▂▂▃▃▁▁▁▁▁▅▆▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▇██▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_epoch</td><td>█▄▁</td></tr><tr><td>val_loss_step</td><td>▇▇▇█▇█▇▇▇██▇▇▇▇▇▇▇▇▄▄▄▄▄▄▄▄▃▄▃▄▄▁▁▁▂▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_loss_epoch</td><td>0.30652</td></tr><tr><td>train_loss_step</td><td>0.27188</td></tr><tr><td>trainer/global_step</td><td>2519</td></tr><tr><td>val_loss_epoch</td><td>0.28209</td></tr><tr><td>val_loss_step</td><td>0.27033</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">type_bs64_lr3.1e-04_wd2.7e-04_optAdamW_schReduceLROnPlateau_acc1_2025-09-04_10-42</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/f1jv7cr7' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/f1jv7cr7</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250904_104204-f1jv7cr7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.282093:  20%|██        | 1/5 [03:22<13:30, 202.70s/it]c:\\Users\\lukas\\anaconda3\\envs\\HKA_EKG_Signalverarbeitung\\Lib\\site-packages\\optuna\\distributions.py:699: UserWarning: The distribution is specified by [3, 5] and step=10, but the range is not divisible by `step`. It will be replaced by [3, 3].\n",
      "  warnings.warn(\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished with best validation loss: 0.28209277987480164\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250904_104526-butfq79i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/butfq79i' target=\"_blank\">type_bs32_lr1.9e-03_wd1.5e-04_optAdamW_schCosineAnnealingLR_acc1_2025-09-04_10-45</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/butfq79i' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/butfq79i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                    | Type                  | Params | Mode  | In sizes      | Out sizes    \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "0  | criterion               | BCEWithLogitsLoss     | 0      | train | ?             | ?            \n",
      "1  | train_jaccard           | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "2  | val_jaccard             | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "3  | test_jaccard            | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "4  | multi_tolerance_metrics | MultiToleranceWrapper | 0      | train | ?             | ?            \n",
      "5  | AvgPool1D1              | AvgPool1d             | 0      | train | [1, 64, 512]  | [1, 64, 256] \n",
      "6  | AvgPool1D2              | AvgPool1d             | 0      | train | [1, 128, 256] | [1, 128, 128]\n",
      "7  | AvgPool1D3              | AvgPool1d             | 0      | train | [1, 256, 128] | [1, 256, 64] \n",
      "8  | AvgPool1D4              | AvgPool1d             | 0      | train | [1, 512, 64]  | [1, 512, 32] \n",
      "9  | layer1                  | Sequential            | 21.2 K | train | [1, 1, 512]   | [1, 64, 512] \n",
      "10 | layer2                  | Sequential            | 123 K  | train | [1, 64, 256]  | [1, 128, 256]\n",
      "11 | layer3                  | Sequential            | 493 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "12 | layer4                  | Sequential            | 2.0 M  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "13 | layer5                  | Sequential            | 7.9 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "14 | layer5T                 | Sequential            | 2.6 M  | train | [1, 1024, 32] | [1, 512, 64] \n",
      "15 | layer4T                 | Sequential            | 3.6 M  | train | [1, 1024, 64] | [1, 256, 128]\n",
      "16 | layer3T                 | Sequential            | 902 K  | train | [1, 512, 128] | [1, 128, 256]\n",
      "17 | layer2T                 | Sequential            | 225 K  | train | [1, 256, 256] | [1, 64, 512] \n",
      "18 | layer1Out               | Sequential            | 43.1 K | train | [1, 128, 512] | [1, 6, 512]  \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "17.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.9 M    Total params\n",
      "71.512    Total estimated model params size (MB)\n",
      "118       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1680/1680 [01:07<00:00, 24.72it/s, v_num=q79i, train_loss_step=0.0675, val_loss_step=0.0681, val_loss_epoch=0.0863, train_loss_epoch=0.0887]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1680/1680 [01:08<00:00, 24.43it/s, v_num=q79i, train_loss_step=0.0675, val_loss_step=0.0681, val_loss_epoch=0.0863, train_loss_epoch=0.0887]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅███████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▂▃▁▁▁▁▁▂▅▆▆▆▂▂▂▂▂▂▂▂▂▂▇▇▇██▂▂▂▂▂▂▂▃▃▃▃▃</td></tr><tr><td>val_loss_epoch</td><td>█▃▁</td></tr><tr><td>val_loss_step</td><td>▅▅▄▄▄▆▆▄▄▃▅▄▄▅▅▃▄▄▂▃▂▃▃▄▂▂▃▂▄▂▂▃▂▃▁▂▃▁▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_loss_epoch</td><td>0.08869</td></tr><tr><td>train_loss_step</td><td>0.06752</td></tr><tr><td>trainer/global_step</td><td>5039</td></tr><tr><td>val_loss_epoch</td><td>0.08629</td></tr><tr><td>val_loss_step</td><td>0.06815</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">type_bs32_lr1.9e-03_wd1.5e-04_optAdamW_schCosineAnnealingLR_acc1_2025-09-04_10-45</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/butfq79i' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/butfq79i</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250904_104526-butfq79i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.0862901:  40%|████      | 2/5 [07:37<11:40, 233.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished with best validation loss: 0.08629006892442703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\anaconda3\\envs\\HKA_EKG_Signalverarbeitung\\Lib\\site-packages\\optuna\\distributions.py:699: UserWarning: The distribution is specified by [3, 5] and step=10, but the range is not divisible by `step`. It will be replaced by [3, 3].\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250904_104941-k8r3w35z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/k8r3w35z' target=\"_blank\">type_bs16_lr1.7e-04_wd3.1e-03_optAdamW_schReduceLROnPlateau_acc8_2025-09-04_10-49</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/k8r3w35z' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/k8r3w35z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                    | Type                  | Params | Mode  | In sizes      | Out sizes    \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "0  | criterion               | BCEWithLogitsLoss     | 0      | train | ?             | ?            \n",
      "1  | train_jaccard           | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "2  | val_jaccard             | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "3  | test_jaccard            | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "4  | multi_tolerance_metrics | MultiToleranceWrapper | 0      | train | ?             | ?            \n",
      "5  | AvgPool1D1              | AvgPool1d             | 0      | train | [1, 64, 512]  | [1, 64, 256] \n",
      "6  | AvgPool1D2              | AvgPool1d             | 0      | train | [1, 128, 256] | [1, 128, 128]\n",
      "7  | AvgPool1D3              | AvgPool1d             | 0      | train | [1, 256, 128] | [1, 256, 64] \n",
      "8  | AvgPool1D4              | AvgPool1d             | 0      | train | [1, 512, 64]  | [1, 512, 32] \n",
      "9  | layer1                  | Sequential            | 21.2 K | train | [1, 1, 512]   | [1, 64, 512] \n",
      "10 | layer2                  | Sequential            | 123 K  | train | [1, 64, 256]  | [1, 128, 256]\n",
      "11 | layer3                  | Sequential            | 493 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "12 | layer4                  | Sequential            | 2.0 M  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "13 | layer5                  | Sequential            | 7.9 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "14 | layer5T                 | Sequential            | 2.6 M  | train | [1, 1024, 32] | [1, 512, 64] \n",
      "15 | layer4T                 | Sequential            | 3.6 M  | train | [1, 1024, 64] | [1, 256, 128]\n",
      "16 | layer3T                 | Sequential            | 902 K  | train | [1, 512, 128] | [1, 128, 256]\n",
      "17 | layer2T                 | Sequential            | 225 K  | train | [1, 256, 256] | [1, 64, 512] \n",
      "18 | layer1Out               | Sequential            | 43.1 K | train | [1, 128, 512] | [1, 6, 512]  \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "17.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.9 M    Total params\n",
      "71.512    Total estimated model params size (MB)\n",
      "118       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3359/3359 [01:51<00:00, 30.23it/s, v_num=w35z, train_loss_step=0.458, val_loss_step=0.419, val_loss_epoch=0.424, train_loss_epoch=0.440]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3359/3359 [01:51<00:00, 30.01it/s, v_num=w35z, train_loss_step=0.458, val_loss_step=0.419, val_loss_epoch=0.424, train_loss_epoch=0.440]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅████████████</td></tr><tr><td>train_loss_epoch</td><td>█▄▁</td></tr><tr><td>train_loss_step</td><td>█▆▅▄▅▄▄▄▄▄▄▄▄▃▄▄▃▃▃▃▃▃▄▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▂▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss_epoch</td><td>█▄▁</td></tr><tr><td>val_loss_step</td><td>▇▆▆█▇▇█▆▆▇▇▆▆▆█▄▄▄▅▄▅▅▅▄▄▅▅▄▆▃█▁▁▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_loss_epoch</td><td>0.44007</td></tr><tr><td>train_loss_step</td><td>0.45831</td></tr><tr><td>trainer/global_step</td><td>1259</td></tr><tr><td>val_loss_epoch</td><td>0.42443</td></tr><tr><td>val_loss_step</td><td>0.41897</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">type_bs16_lr1.7e-04_wd3.1e-03_optAdamW_schReduceLROnPlateau_acc8_2025-09-04_10-49</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/k8r3w35z' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/k8r3w35z</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250904_104941-k8r3w35z\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.0862901:  60%|██████    | 3/5 [13:50<09:54, 297.03s/it]c:\\Users\\lukas\\anaconda3\\envs\\HKA_EKG_Signalverarbeitung\\Lib\\site-packages\\optuna\\distributions.py:699: UserWarning: The distribution is specified by [3, 5] and step=10, but the range is not divisible by `step`. It will be replaced by [3, 3].\n",
      "  warnings.warn(\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished with best validation loss: 0.42443081736564636\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250904_105554-aniijlpi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/aniijlpi' target=\"_blank\">type_bs64_lr1.7e-03_wd4.7e-05_optAdam_schReduceLROnPlateau_acc8_2025-09-04_10-55</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/aniijlpi' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/aniijlpi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                    | Type                  | Params | Mode  | In sizes      | Out sizes    \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "0  | criterion               | BCEWithLogitsLoss     | 0      | train | ?             | ?            \n",
      "1  | train_jaccard           | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "2  | val_jaccard             | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "3  | test_jaccard            | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "4  | multi_tolerance_metrics | MultiToleranceWrapper | 0      | train | ?             | ?            \n",
      "5  | AvgPool1D1              | AvgPool1d             | 0      | train | [1, 64, 512]  | [1, 64, 256] \n",
      "6  | AvgPool1D2              | AvgPool1d             | 0      | train | [1, 128, 256] | [1, 128, 128]\n",
      "7  | AvgPool1D3              | AvgPool1d             | 0      | train | [1, 256, 128] | [1, 256, 64] \n",
      "8  | AvgPool1D4              | AvgPool1d             | 0      | train | [1, 512, 64]  | [1, 512, 32] \n",
      "9  | layer1                  | Sequential            | 21.2 K | train | [1, 1, 512]   | [1, 64, 512] \n",
      "10 | layer2                  | Sequential            | 123 K  | train | [1, 64, 256]  | [1, 128, 256]\n",
      "11 | layer3                  | Sequential            | 493 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "12 | layer4                  | Sequential            | 2.0 M  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "13 | layer5                  | Sequential            | 7.9 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "14 | layer5T                 | Sequential            | 2.6 M  | train | [1, 1024, 32] | [1, 512, 64] \n",
      "15 | layer4T                 | Sequential            | 3.6 M  | train | [1, 1024, 64] | [1, 256, 128]\n",
      "16 | layer3T                 | Sequential            | 902 K  | train | [1, 512, 128] | [1, 128, 256]\n",
      "17 | layer2T                 | Sequential            | 225 K  | train | [1, 256, 256] | [1, 64, 512] \n",
      "18 | layer1Out               | Sequential            | 43.1 K | train | [1, 128, 512] | [1, 6, 512]  \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "17.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.9 M    Total params\n",
      "71.512    Total estimated model params size (MB)\n",
      "118       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 840/840 [00:42<00:00, 19.95it/s, v_num=jlpi, train_loss_step=0.351, val_loss_step=0.334, val_loss_epoch=0.346, train_loss_epoch=0.366]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 840/840 [00:42<00:00, 19.66it/s, v_num=jlpi, train_loss_step=0.351, val_loss_step=0.334, val_loss_epoch=0.346, train_loss_epoch=0.366]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅████████████</td></tr><tr><td>train_loss_epoch</td><td>█▄▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▂▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▄▆▆▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▅▁</td></tr><tr><td>val_loss_step</td><td>▇▇▇█▆▇▇▇▇▇▇▇█▇▄▅▄▅▅▄▄▅▄▄▂▁▂▂▁▂▂▁▁▁▂▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_loss_epoch</td><td>0.36616</td></tr><tr><td>train_loss_step</td><td>0.34943</td></tr><tr><td>trainer/global_step</td><td>314</td></tr><tr><td>val_loss_epoch</td><td>0.34598</td></tr><tr><td>val_loss_step</td><td>0.33382</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">type_bs64_lr1.7e-03_wd4.7e-05_optAdam_schReduceLROnPlateau_acc8_2025-09-04_10-55</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/aniijlpi' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/aniijlpi</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250904_105554-aniijlpi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.0862901:  80%|████████  | 4/5 [16:54<04:12, 252.62s/it]c:\\Users\\lukas\\anaconda3\\envs\\HKA_EKG_Signalverarbeitung\\Lib\\site-packages\\optuna\\distributions.py:699: UserWarning: The distribution is specified by [3, 5] and step=10, but the range is not divisible by `step`. It will be replaced by [3, 3].\n",
      "  warnings.warn(\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished with best validation loss: 0.3459818959236145\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250904_105858-fpkuwf0j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/fpkuwf0j' target=\"_blank\">type_bs64_lr4.7e-04_wd4.2e-04_optAdam_schReduceLROnPlateau_acc4_2025-09-04_10-58</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/fpkuwf0j' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/fpkuwf0j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                    | Type                  | Params | Mode  | In sizes      | Out sizes    \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "0  | criterion               | BCEWithLogitsLoss     | 0      | train | ?             | ?            \n",
      "1  | train_jaccard           | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "2  | val_jaccard             | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "3  | test_jaccard            | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "4  | multi_tolerance_metrics | MultiToleranceWrapper | 0      | train | ?             | ?            \n",
      "5  | AvgPool1D1              | AvgPool1d             | 0      | train | [1, 64, 512]  | [1, 64, 256] \n",
      "6  | AvgPool1D2              | AvgPool1d             | 0      | train | [1, 128, 256] | [1, 128, 128]\n",
      "7  | AvgPool1D3              | AvgPool1d             | 0      | train | [1, 256, 128] | [1, 256, 64] \n",
      "8  | AvgPool1D4              | AvgPool1d             | 0      | train | [1, 512, 64]  | [1, 512, 32] \n",
      "9  | layer1                  | Sequential            | 21.2 K | train | [1, 1, 512]   | [1, 64, 512] \n",
      "10 | layer2                  | Sequential            | 123 K  | train | [1, 64, 256]  | [1, 128, 256]\n",
      "11 | layer3                  | Sequential            | 493 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "12 | layer4                  | Sequential            | 2.0 M  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "13 | layer5                  | Sequential            | 7.9 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "14 | layer5T                 | Sequential            | 2.6 M  | train | [1, 1024, 32] | [1, 512, 64] \n",
      "15 | layer4T                 | Sequential            | 3.6 M  | train | [1, 1024, 64] | [1, 256, 128]\n",
      "16 | layer3T                 | Sequential            | 902 K  | train | [1, 512, 128] | [1, 128, 256]\n",
      "17 | layer2T                 | Sequential            | 225 K  | train | [1, 256, 256] | [1, 64, 512] \n",
      "18 | layer1Out               | Sequential            | 43.1 K | train | [1, 128, 512] | [1, 6, 512]  \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "17.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.9 M    Total params\n",
      "71.512    Total estimated model params size (MB)\n",
      "118       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 840/840 [00:42<00:00, 19.58it/s, v_num=wf0j, train_loss_step=0.400, val_loss_step=0.392, val_loss_epoch=0.406, train_loss_epoch=0.421]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 840/840 [00:43<00:00, 19.19it/s, v_num=wf0j, train_loss_step=0.400, val_loss_step=0.392, val_loss_epoch=0.406, train_loss_epoch=0.421]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>train_loss_epoch</td><td>█▄▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▄▃▃▃▄▄▅▅▅▅▅▅▅▇█▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▄▁</td></tr><tr><td>val_loss_step</td><td>▇▇█▇█▇▇▇▆▇▇▇█▇▇█▇▆▅▄▄▄▄▅▅▄▄▃▂▂▂▅▁▁▁▂▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_loss_epoch</td><td>0.42099</td></tr><tr><td>train_loss_step</td><td>0.39982</td></tr><tr><td>trainer/global_step</td><td>629</td></tr><tr><td>val_loss_epoch</td><td>0.40584</td></tr><tr><td>val_loss_step</td><td>0.39163</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">type_bs64_lr4.7e-04_wd4.2e-04_optAdam_schReduceLROnPlateau_acc4_2025-09-04_10-58</strong> at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/fpkuwf0j' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/fpkuwf0j</a><br> View project at: <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250904_105858-fpkuwf0j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.0862901: 100%|██████████| 5/5 [20:02<00:00, 240.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished with best validation loss: 0.4058423638343811\n",
      "Best trial:  FrozenTrial(number=1, state=1, values=[0.08629006892442703], datetime_start=datetime.datetime(2025, 9, 4, 10, 45, 26, 739187), datetime_complete=datetime.datetime(2025, 9, 4, 10, 49, 41, 373948), params={'batch_size': 32, 'max_epochs': 3, 'accumulate_grad_batches': 1, 'precision': 32, 'optimizer': 'AdamW', 'learning_rate': 0.001874583495068444, 'weight_decay': 0.00015012419395533578, 'scheduler': 'CosineAnnealingLR'}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'batch_size': CategoricalDistribution(choices=(16, 32, 64)), 'max_epochs': IntDistribution(high=3, log=False, low=3, step=10), 'accumulate_grad_batches': CategoricalDistribution(choices=(1, 2, 4, 8)), 'precision': CategoricalDistribution(choices=('16-mixed', 32)), 'optimizer': CategoricalDistribution(choices=('Adam', 'SGD', 'AdamW')), 'learning_rate': FloatDistribution(high=0.01, log=True, low=5e-05, step=None), 'weight_decay': FloatDistribution(high=0.01, log=True, low=1e-05, step=None), 'scheduler': CategoricalDistribution(choices=('StepLR', 'CosineAnnealingLR', 'ReduceLROnPlateau'))}, trial_id=1, value=None)\n",
      "Best value (loss):  0.08629006892442703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from training.hyperparameter_optimization import OptunaTrainer\n",
    "\n",
    "def objective(trial):\n",
    "    model = UNET_1D\n",
    "    config[\"sweep_id\"] = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    config[\"dataset_name\"] = \"ECG_TEST_DATASET\"\n",
    "    config[\"model_name\"] = type(model).__name__\n",
    "    trainer = OptunaTrainer(\n",
    "        model=model,\n",
    "        config=config\n",
    "    )\n",
    "    return trainer.run_training(trial)\n",
    "\n",
    "# Optuna Hyperparameter Study\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=f\"Optuna_HPO_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\")\n",
    "\n",
    "# Reduce output clutter by setting verbosity to WARNING\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Start optimization\n",
    "study.optimize(objective, n_trials=config['number_of_trials'], gc_after_trial=True, show_progress_bar=True)\n",
    "\n",
    "# Best result\n",
    "print(\"Best trial: \", study.best_trial)\n",
    "print(\"Best value (loss): \", study.best_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HKA_EKG_Signalverarbeitung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

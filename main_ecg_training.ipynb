{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3fa61c3",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcdc5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "\n",
    "from config.load_configuration import load_configuration\n",
    "from data.datamodule import ECG_DataModule\n",
    "from model.model import UNET_1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d352ac",
   "metadata": {},
   "source": [
    "#### Loading configuration\n",
    "\n",
    "This notebook loads configuration settings using the `load_configuration` function from the `config.load_configuration` module. The configuration is stored in the `config` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a036c93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC Name: DESKTOP-LUKAS\n",
      "Loaded configuration from config/config_lukas.yaml\n"
     ]
    }
   ],
   "source": [
    "config = load_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ef236e",
   "metadata": {},
   "source": [
    "#### Logging in to Weights & Biases (wandb)\n",
    "\n",
    "Before starting any experiment tracking, ensure you are logged in to your Weights & Biases (wandb) account. This enables automatic logging of metrics, model checkpoints, and experiment configurations. The following code logs you in to wandb:\n",
    "\n",
    "```python\n",
    "wandb.login()\n",
    "```\n",
    "If you are running this for the first time, you may be prompted to enter your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375929d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukas-pelz\u001b[0m (\u001b[33mHKA-EKG-Signalverarbeitung\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a2bf1",
   "metadata": {},
   "source": [
    "#### Setting Seeds for Reproducibility\n",
    "\n",
    "To ensure comparable and reproducible results, we set the random seed using the `seed_everything` function from PyTorch Lightning. This helps in achieving consistent behavior across multiple runs of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e672fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(config['seed'])\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"   # disable oneDNN optimizations for reproducibility\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e7fa5",
   "metadata": {},
   "source": [
    "#### Checking for GPU Devices\n",
    "\n",
    "In this step, we check for the availability of GPU devices and print the device currently being used by PyTorch. This ensures that the computations are performed on the most efficient hardware available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f0450c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Torch Version      : 2.7.0+cu128\n",
      "Selected Device    : cuda\n",
      "CUDA Version       : 12.8\n",
      "Device Name        : NVIDIA GeForce RTX 5060 Ti\n",
      "Memory Usage       : Allocated: 0.00 GB | Reserved: 0.00 GB\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"=\"*40)\n",
    "print(f\"Torch Version      : {torch.__version__}\")\n",
    "print(f\"Selected Device    : {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"CUDA Version       : {torch.version.cuda}\")\n",
    "    print(f\"Device Name        : {torch.cuda.get_device_name(0)}\")\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"Memory Usage       : Allocated: {allocated:.2f} GB | Reserved: {reserved:.2f} GB\")\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU.\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40cb2e8",
   "metadata": {},
   "source": [
    "#### Initializing the Data Module\n",
    "\n",
    "The `ECG_DataModule` is initialized using the data path, batch size, and feature list from the configuration. This prepares the data for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a53c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ECG_DataModule(data_dir=config['path_to_data'], batch_size=config['batch_size'], feature_list=config['feature_list'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0693029f",
   "metadata": {},
   "source": [
    "#### Creating the Model\n",
    "\n",
    "In this step, we will define the model architecture and print its summary using the `ModelSummary` utility from PyTorch Lightning. This provides an overview of the model's layers, parameters, and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "038f7c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | Name                                            | Type                  | Params | Mode  | In sizes      | Out sizes    \n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "0   | criterion                                       | BCEWithLogitsLoss     | 0      | train | ?             | ?            \n",
      "1   | train_jaccard                                   | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "2   | val_jaccard                                     | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "3   | test_jaccard                                    | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "4   | multi_tolerance_metrics                         | MultiToleranceWrapper | 0      | train | ?             | ?            \n",
      "5   | multi_tolerance_metrics.metrics                 | ModuleDict            | 0      | train | ?             | ?            \n",
      "6   | multi_tolerance_metrics.metrics.tolerance_5ms   | CustomMetrics         | 0      | train | ?             | ?            \n",
      "7   | multi_tolerance_metrics.metrics.tolerance_10ms  | CustomMetrics         | 0      | train | ?             | ?            \n",
      "8   | multi_tolerance_metrics.metrics.tolerance_40ms  | CustomMetrics         | 0      | train | ?             | ?            \n",
      "9   | multi_tolerance_metrics.metrics.tolerance_150ms | CustomMetrics         | 0      | train | ?             | ?            \n",
      "10  | AvgPool1D1                                      | AvgPool1d             | 0      | train | [1, 64, 512]  | [1, 64, 256] \n",
      "11  | AvgPool1D2                                      | AvgPool1d             | 0      | train | [1, 128, 256] | [1, 128, 128]\n",
      "12  | AvgPool1D3                                      | AvgPool1d             | 0      | train | [1, 256, 128] | [1, 256, 64] \n",
      "13  | AvgPool1D4                                      | AvgPool1d             | 0      | train | [1, 512, 64]  | [1, 512, 32] \n",
      "14  | layer1                                          | Sequential            | 21.2 K | train | [1, 1, 512]   | [1, 64, 512] \n",
      "15  | layer1.0                                        | conbr_block           | 512    | train | [1, 1, 512]   | [1, 64, 512] \n",
      "16  | layer1.0.net                                    | Sequential            | 512    | train | [1, 1, 512]   | [1, 64, 512] \n",
      "17  | layer1.0.net.0                                  | Conv1d                | 384    | train | [1, 1, 512]   | [1, 64, 512] \n",
      "18  | layer1.0.net.1                                  | ReLU                  | 0      | train | [1, 64, 512]  | [1, 64, 512] \n",
      "19  | layer1.0.net.2                                  | BatchNorm1d           | 128    | train | [1, 64, 512]  | [1, 64, 512] \n",
      "20  | layer1.1                                        | conbr_block           | 20.7 K | train | [1, 64, 512]  | [1, 64, 512] \n",
      "21  | layer1.1.net                                    | Sequential            | 20.7 K | train | [1, 64, 512]  | [1, 64, 512] \n",
      "22  | layer1.1.net.0                                  | Conv1d                | 20.5 K | train | [1, 64, 512]  | [1, 64, 512] \n",
      "23  | layer1.1.net.1                                  | ReLU                  | 0      | train | [1, 64, 512]  | [1, 64, 512] \n",
      "24  | layer1.1.net.2                                  | BatchNorm1d           | 128    | train | [1, 64, 512]  | [1, 64, 512] \n",
      "25  | layer2                                          | Sequential            | 123 K  | train | [1, 64, 256]  | [1, 128, 256]\n",
      "26  | layer2.0                                        | conbr_block           | 41.3 K | train | [1, 64, 256]  | [1, 128, 256]\n",
      "27  | layer2.0.net                                    | Sequential            | 41.3 K | train | [1, 64, 256]  | [1, 128, 256]\n",
      "28  | layer2.0.net.0                                  | Conv1d                | 41.1 K | train | [1, 64, 256]  | [1, 128, 256]\n",
      "29  | layer2.0.net.1                                  | ReLU                  | 0      | train | [1, 128, 256] | [1, 128, 256]\n",
      "30  | layer2.0.net.2                                  | BatchNorm1d           | 256    | train | [1, 128, 256] | [1, 128, 256]\n",
      "31  | layer2.1                                        | conbr_block           | 82.3 K | train | [1, 128, 256] | [1, 128, 256]\n",
      "32  | layer2.1.net                                    | Sequential            | 82.3 K | train | [1, 128, 256] | [1, 128, 256]\n",
      "33  | layer2.1.net.0                                  | Conv1d                | 82.0 K | train | [1, 128, 256] | [1, 128, 256]\n",
      "34  | layer2.1.net.1                                  | ReLU                  | 0      | train | [1, 128, 256] | [1, 128, 256]\n",
      "35  | layer2.1.net.2                                  | BatchNorm1d           | 256    | train | [1, 128, 256] | [1, 128, 256]\n",
      "36  | layer3                                          | Sequential            | 493 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "37  | layer3.0                                        | conbr_block           | 164 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "38  | layer3.0.net                                    | Sequential            | 164 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "39  | layer3.0.net.0                                  | Conv1d                | 164 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "40  | layer3.0.net.1                                  | ReLU                  | 0      | train | [1, 256, 128] | [1, 256, 128]\n",
      "41  | layer3.0.net.2                                  | BatchNorm1d           | 512    | train | [1, 256, 128] | [1, 256, 128]\n",
      "42  | layer3.1                                        | conbr_block           | 328 K  | train | [1, 256, 128] | [1, 256, 128]\n",
      "43  | layer3.1.net                                    | Sequential            | 328 K  | train | [1, 256, 128] | [1, 256, 128]\n",
      "44  | layer3.1.net.0                                  | Conv1d                | 327 K  | train | [1, 256, 128] | [1, 256, 128]\n",
      "45  | layer3.1.net.1                                  | ReLU                  | 0      | train | [1, 256, 128] | [1, 256, 128]\n",
      "46  | layer3.1.net.2                                  | BatchNorm1d           | 512    | train | [1, 256, 128] | [1, 256, 128]\n",
      "47  | layer4                                          | Sequential            | 2.0 M  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "48  | layer4.0                                        | conbr_block           | 656 K  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "49  | layer4.0.net                                    | Sequential            | 656 K  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "50  | layer4.0.net.0                                  | Conv1d                | 655 K  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "51  | layer4.0.net.1                                  | ReLU                  | 0      | train | [1, 512, 64]  | [1, 512, 64] \n",
      "52  | layer4.0.net.2                                  | BatchNorm1d           | 1.0 K  | train | [1, 512, 64]  | [1, 512, 64] \n",
      "53  | layer4.1                                        | conbr_block           | 1.3 M  | train | [1, 512, 64]  | [1, 512, 64] \n",
      "54  | layer4.1.net                                    | Sequential            | 1.3 M  | train | [1, 512, 64]  | [1, 512, 64] \n",
      "55  | layer4.1.net.0                                  | Conv1d                | 1.3 M  | train | [1, 512, 64]  | [1, 512, 64] \n",
      "56  | layer4.1.net.1                                  | ReLU                  | 0      | train | [1, 512, 64]  | [1, 512, 64] \n",
      "57  | layer4.1.net.2                                  | BatchNorm1d           | 1.0 K  | train | [1, 512, 64]  | [1, 512, 64] \n",
      "58  | layer5                                          | Sequential            | 7.9 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "59  | layer5.0                                        | conbr_block           | 2.6 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "60  | layer5.0.net                                    | Sequential            | 2.6 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "61  | layer5.0.net.0                                  | Conv1d                | 2.6 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "62  | layer5.0.net.1                                  | ReLU                  | 0      | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "63  | layer5.0.net.2                                  | BatchNorm1d           | 2.0 K  | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "64  | layer5.1                                        | conbr_block           | 5.2 M  | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "65  | layer5.1.net                                    | Sequential            | 5.2 M  | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "66  | layer5.1.net.0                                  | Conv1d                | 5.2 M  | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "67  | layer5.1.net.1                                  | ReLU                  | 0      | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "68  | layer5.1.net.2                                  | BatchNorm1d           | 2.0 K  | train | [1, 1024, 32] | [1, 1024, 32]\n",
      "69  | layer5T                                         | Sequential            | 2.6 M  | train | [1, 1024, 32] | [1, 512, 64] \n",
      "70  | layer5T.0                                       | ConvTranspose1d       | 2.6 M  | train | [1, 1024, 32] | [1, 512, 64] \n",
      "71  | layer4T                                         | Sequential            | 3.6 M  | train | [1, 1024, 64] | [1, 256, 128]\n",
      "72  | layer4T.0                                       | conbr_block           | 2.6 M  | train | [1, 1024, 64] | [1, 512, 64] \n",
      "73  | layer4T.0.net                                   | Sequential            | 2.6 M  | train | [1, 1024, 64] | [1, 512, 64] \n",
      "74  | layer4T.0.net.0                                 | Conv1d                | 2.6 M  | train | [1, 1024, 64] | [1, 512, 64] \n",
      "75  | layer4T.0.net.1                                 | ReLU                  | 0      | train | [1, 512, 64]  | [1, 512, 64] \n",
      "76  | layer4T.0.net.2                                 | BatchNorm1d           | 1.0 K  | train | [1, 512, 64]  | [1, 512, 64] \n",
      "77  | layer4T.1                                       | conbr_block           | 656 K  | train | [1, 512, 64]  | [1, 256, 64] \n",
      "78  | layer4T.1.net                                   | Sequential            | 656 K  | train | [1, 512, 64]  | [1, 256, 64] \n",
      "79  | layer4T.1.net.0                                 | Conv1d                | 655 K  | train | [1, 512, 64]  | [1, 256, 64] \n",
      "80  | layer4T.1.net.1                                 | ReLU                  | 0      | train | [1, 256, 64]  | [1, 256, 64] \n",
      "81  | layer4T.1.net.2                                 | BatchNorm1d           | 512    | train | [1, 256, 64]  | [1, 256, 64] \n",
      "82  | layer4T.2                                       | ConvTranspose1d       | 327 K  | train | [1, 256, 64]  | [1, 256, 128]\n",
      "83  | layer3T                                         | Sequential            | 902 K  | train | [1, 512, 128] | [1, 128, 256]\n",
      "84  | layer3T.0                                       | conbr_block           | 656 K  | train | [1, 512, 128] | [1, 256, 128]\n",
      "85  | layer3T.0.net                                   | Sequential            | 656 K  | train | [1, 512, 128] | [1, 256, 128]\n",
      "86  | layer3T.0.net.0                                 | Conv1d                | 655 K  | train | [1, 512, 128] | [1, 256, 128]\n",
      "87  | layer3T.0.net.1                                 | ReLU                  | 0      | train | [1, 256, 128] | [1, 256, 128]\n",
      "88  | layer3T.0.net.2                                 | BatchNorm1d           | 512    | train | [1, 256, 128] | [1, 256, 128]\n",
      "89  | layer3T.1                                       | conbr_block           | 164 K  | train | [1, 256, 128] | [1, 128, 128]\n",
      "90  | layer3T.1.net                                   | Sequential            | 164 K  | train | [1, 256, 128] | [1, 128, 128]\n",
      "91  | layer3T.1.net.0                                 | Conv1d                | 163 K  | train | [1, 256, 128] | [1, 128, 128]\n",
      "92  | layer3T.1.net.1                                 | ReLU                  | 0      | train | [1, 128, 128] | [1, 128, 128]\n",
      "93  | layer3T.1.net.2                                 | BatchNorm1d           | 256    | train | [1, 128, 128] | [1, 128, 128]\n",
      "94  | layer3T.2                                       | ConvTranspose1d       | 82.0 K | train | [1, 128, 128] | [1, 128, 256]\n",
      "95  | layer2T                                         | Sequential            | 225 K  | train | [1, 256, 256] | [1, 64, 512] \n",
      "96  | layer2T.0                                       | conbr_block           | 164 K  | train | [1, 256, 256] | [1, 128, 256]\n",
      "97  | layer2T.0.net                                   | Sequential            | 164 K  | train | [1, 256, 256] | [1, 128, 256]\n",
      "98  | layer2T.0.net.0                                 | Conv1d                | 163 K  | train | [1, 256, 256] | [1, 128, 256]\n",
      "99  | layer2T.0.net.1                                 | ReLU                  | 0      | train | [1, 128, 256] | [1, 128, 256]\n",
      "100 | layer2T.0.net.2                                 | BatchNorm1d           | 256    | train | [1, 128, 256] | [1, 128, 256]\n",
      "101 | layer2T.1                                       | conbr_block           | 41.2 K | train | [1, 128, 256] | [1, 64, 256] \n",
      "102 | layer2T.1.net                                   | Sequential            | 41.2 K | train | [1, 128, 256] | [1, 64, 256] \n",
      "103 | layer2T.1.net.0                                 | Conv1d                | 41.0 K | train | [1, 128, 256] | [1, 64, 256] \n",
      "104 | layer2T.1.net.1                                 | ReLU                  | 0      | train | [1, 64, 256]  | [1, 64, 256] \n",
      "105 | layer2T.1.net.2                                 | BatchNorm1d           | 128    | train | [1, 64, 256]  | [1, 64, 256] \n",
      "106 | layer2T.2                                       | ConvTranspose1d       | 20.5 K | train | [1, 64, 256]  | [1, 64, 512] \n",
      "107 | layer1Out                                       | Sequential            | 43.1 K | train | [1, 128, 512] | [1, 6, 512]  \n",
      "108 | layer1Out.0                                     | conbr_block           | 41.2 K | train | [1, 128, 512] | [1, 64, 512] \n",
      "109 | layer1Out.0.net                                 | Sequential            | 41.2 K | train | [1, 128, 512] | [1, 64, 512] \n",
      "110 | layer1Out.0.net.0                               | Conv1d                | 41.0 K | train | [1, 128, 512] | [1, 64, 512] \n",
      "111 | layer1Out.0.net.1                               | ReLU                  | 0      | train | [1, 64, 512]  | [1, 64, 512] \n",
      "112 | layer1Out.0.net.2                               | BatchNorm1d           | 128    | train | [1, 64, 512]  | [1, 64, 512] \n",
      "113 | layer1Out.1                                     | conbr_block           | 1.9 K  | train | [1, 64, 512]  | [1, 6, 512]  \n",
      "114 | layer1Out.1.net                                 | Sequential            | 1.9 K  | train | [1, 64, 512]  | [1, 6, 512]  \n",
      "115 | layer1Out.1.net.0                               | Conv1d                | 1.9 K  | train | [1, 64, 512]  | [1, 6, 512]  \n",
      "116 | layer1Out.1.net.1                               | ReLU                  | 0      | train | [1, 6, 512]   | [1, 6, 512]  \n",
      "117 | layer1Out.1.net.2                               | BatchNorm1d           | 12     | train | [1, 6, 512]   | [1, 6, 512]  \n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "17.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.9 M    Total params\n",
      "71.512    Total estimated model params size (MB)\n",
      "118       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "UNET_1D\n"
     ]
    }
   ],
   "source": [
    "model = UNET_1D(in_channels=1, layer_n=512, out_channels=6, kernel_size=5)\n",
    "print(ModelSummary(model, max_depth=-1))  \n",
    "print(type(model).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a000edf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time       : 2025-09-02_14-45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20250902_144510-0tpsihpm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/0tpsihpm' target=\"_blank\">1-lead_UNET_1D_2025-09-02_14-45</a></strong> to <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/0tpsihpm' target=\"_blank\">https://wandb.ai/HKA-EKG-Signalverarbeitung/HKA-EKG-Signalverarbeitung/runs/0tpsihpm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                    | Type                  | Params | Mode  | In sizes      | Out sizes    \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "0  | criterion               | BCEWithLogitsLoss     | 0      | train | ?             | ?            \n",
      "1  | train_jaccard           | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "2  | val_jaccard             | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "3  | test_jaccard            | BinaryJaccardIndex    | 0      | train | ?             | ?            \n",
      "4  | multi_tolerance_metrics | MultiToleranceWrapper | 0      | train | ?             | ?            \n",
      "5  | AvgPool1D1              | AvgPool1d             | 0      | train | [1, 64, 512]  | [1, 64, 256] \n",
      "6  | AvgPool1D2              | AvgPool1d             | 0      | train | [1, 128, 256] | [1, 128, 128]\n",
      "7  | AvgPool1D3              | AvgPool1d             | 0      | train | [1, 256, 128] | [1, 256, 64] \n",
      "8  | AvgPool1D4              | AvgPool1d             | 0      | train | [1, 512, 64]  | [1, 512, 32] \n",
      "9  | layer1                  | Sequential            | 21.2 K | train | [1, 1, 512]   | [1, 64, 512] \n",
      "10 | layer2                  | Sequential            | 123 K  | train | [1, 64, 256]  | [1, 128, 256]\n",
      "11 | layer3                  | Sequential            | 493 K  | train | [1, 128, 128] | [1, 256, 128]\n",
      "12 | layer4                  | Sequential            | 2.0 M  | train | [1, 256, 64]  | [1, 512, 64] \n",
      "13 | layer5                  | Sequential            | 7.9 M  | train | [1, 512, 32]  | [1, 1024, 32]\n",
      "14 | layer5T                 | Sequential            | 2.6 M  | train | [1, 1024, 32] | [1, 512, 64] \n",
      "15 | layer4T                 | Sequential            | 3.6 M  | train | [1, 1024, 64] | [1, 256, 128]\n",
      "16 | layer3T                 | Sequential            | 902 K  | train | [1, 512, 128] | [1, 128, 256]\n",
      "17 | layer2T                 | Sequential            | 225 K  | train | [1, 256, 256] | [1, 64, 512] \n",
      "18 | layer1Out               | Sequential            | 43.1 K | train | [1, 128, 512] | [1, 6, 512]  \n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "17.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.9 M    Total params\n",
      "71.512    Total estimated model params size (MB)\n",
      "118       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  76%|███████▌  | 2556/3359 [01:13<00:23, 34.77it/s, v_num=ihpm, train_loss_step=0.0936, val_loss_step=0.104, val_loss_epoch=0.122, train_loss_epoch=0.228]"
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "print(f\"Current Time       : {current_time}\")\n",
    "\n",
    "# Initialize wandb logger (https://wandb.ai/HKA-EKG-Signalverarbeitung)\n",
    "wandb_logger = WandbLogger(\n",
    "    project=config['wandb_project_name'],\n",
    "    name=f\"{config['wandb_experiment_name']}_{type(model).__name__}_{current_time}\",\n",
    "    config={\n",
    "        'model': type(model).__name__,\n",
    "        'dataset': type(dm).__name__,\n",
    "        'batch_size': config['batch_size'],\n",
    "        'max_epochs': config['max_epochs'],\n",
    "        'learning_rate': config['learning_rate']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize Trainer with wandb logger, using early stopping callback (https://lightning.ai/docs/pytorch/stable/common/early_stopping.html)\n",
    "trainer = Trainer(\n",
    "    max_epochs=config['max_epochs'], \n",
    "    default_root_dir='model/checkpoint/', #data_directory, \n",
    "    accelerator=\"auto\", \n",
    "    devices=\"auto\", \n",
    "    strategy=\"auto\",\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, mode='min')], \n",
    "    logger=wandb_logger)\n",
    "\n",
    "trainer.fit(model=model, datamodule=dm)\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish()\n",
    "\n",
    "# Create a filename with date identifier\n",
    "model_filename = f\"{config['wandb_experiment_name']}_{type(model).__name__}_{current_time}.ckpt\"\n",
    "\n",
    "# Save the model's state_dict to the path specified in config\n",
    "save_path = os.path.join(config['path_to_models'], model_filename)\n",
    "trainer.save_checkpoint(save_path)\n",
    "print(f\"Model checkpoint saved as {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HKA_EKG_Signalverarbeitung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
